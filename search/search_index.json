{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"references/","title":"\ud83e\udde0 PPO and Reward Models in LLM Training","text":""},{"location":"references/#1-overview","title":"1. Overview","text":"<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm widely used in fine-tuning Large Language Models (LLMs) under the Reinforcement Learning from Human Feedback (RLHF) framework. It helps bridge the gap between human preferences and LLM outputs by optimizing the model\u2019s responses to align with what humans find helpful, safe, or relevant.</p>"},{"location":"references/#2-the-big-picture-rlhf-pipeline","title":"2. The Big Picture: RLHF Pipeline","text":"<p>RLHF typically follows three key stages:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT)    Train a base LLM on high-quality human demonstration data (prompt\u2013response pairs).</p> </li> <li> <p>Reward Model (RM) Training    Train a separate model to predict a scalar reward from human preference data. For example, given two responses to the same prompt, the model learns which one humans prefer.</p> </li> <li> <p>Reinforcement Learning (e.g., PPO)    Use PPO to fine-tune the SFT model (the \u201cpolicy\u201d) so that it generates outputs that maximize the predicted reward given by the RM.</p> </li> </ol>"},{"location":"references/#3-intuitive-understanding-of-ppo","title":"3. Intuitive Understanding of PPO","text":"<p>Imagine you have an assistant (the LLM) that gives answers. You ask multiple questions, and a teacher (the reward model) scores how good each answer is. PPO helps you adjust the assistant\u2019s behavior to get higher scores without deviating too far from its original style (the SFT model).</p> <p>The \u201cproximal\u201d in PPO means:  </p> <p>\u201cDon\u2019t change the model\u2019s behavior too drastically in one update.\u201d</p> <p>This stability comes from a clipped objective function that penalizes large policy updates.</p>"},{"location":"references/#4-ppo-in-simple-terms","title":"4. PPO in Simple Terms","text":"<ol> <li> <p>Policy model (the LLM):    Generates a response (sequence of tokens) given a prompt.</p> </li> <li> <p>Reward model:    Assigns a reward (score) to the response.</p> </li> <li> <p>Old policy (reference model):    A frozen snapshot of the model before PPO updates \u2014 used to measure how much the new model diverges.</p> </li> <li> <p>PPO update: </p> </li> <li>Compare the new model\u2019s likelihood of the generated tokens to the old model\u2019s likelihood.  </li> <li>Compute the reward advantage (how much better the new behavior is).  </li> <li>Clip the ratio of new/old probabilities to prevent large steps.</li> </ol>"},{"location":"references/#5-ppo-objective-function","title":"5. PPO Objective Function","text":"<p>[ L^{PPO}(\\theta) = \\mathbb{E}_t \\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right] ]</p> <p>Where: - (r_t(\\theta)) = ratio between new and old policy probabilities. - (A_t) = advantage function (how much better an action is than expected). - (\\epsilon) = clipping threshold (often 0.1\u20130.3).</p> <p>This ensures small but stable improvements.</p>"},{"location":"references/#6-example-of-ppo-in-practice","title":"6. Example of PPO in Practice","text":"<pre><code>for prompt in prompts:\n    # 1. Generate response\n    response = policy_model.generate(prompt)\n\n    # 2. Compute reward from reward model\n    reward = reward_model(prompt, response)\n\n    # 3. Compute log probabilities from old and new policies\n    logprobs_old = ref_model.logprobs(prompt, response)\n    logprobs_new = policy_model.logprobs(prompt, response)\n\n    # 4. Compute advantage and clipped objective\n    ratio = torch.exp(logprobs_new - logprobs_old)\n    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n    loss = -torch.mean(torch.min(ratio * advantage, clipped_ratio * advantage))\n\n    # 5. Backpropagate and update model\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"references/#7-why-ppo-instead-of-direct-human-feedback","title":"7. Why PPO Instead of Direct Human Feedback?","text":"<p>Because human annotation is limited and noisy: - We can\u2019t rate every output of a large model. - Direct training on human labels doesn\u2019t handle the credit assignment problem \u2014 which part of the response caused the high or low rating? - PPO allows the model to learn generalizable feedback signals through the reward model, without human labels for every step.</p>"},{"location":"references/#8-limitations-and-challenges-of-ppo-in-llm-training","title":"8. Limitations and Challenges of PPO in LLM Training","text":""},{"location":"references/#1-kl-divergence-penalty-instability","title":"\ud83e\udde9 1. KL Divergence Penalty Instability","text":"<p>To ensure the fine-tuned model doesn\u2019t drift too far from the base model, PPO adds a KL penalty: [ L = L^{PPO} - \\beta D_{KL}(\\pi_{\\theta} || \\pi_{\\text{ref}}) ] However: - Choosing the right KL coefficient ((\\beta)) is tricky. - Too small \u2192 model diverges, harming coherence/safety. - Too large \u2192 model learns too little (under-updates).</p> <p>Modern techniques (e.g., adaptive KL control) try to auto-adjust this balance.</p>"},{"location":"references/#2-high-training-time-and-cost","title":"\u23f3 2. High Training Time and Cost","text":"<ul> <li>PPO fine-tuning is computationally expensive, requiring multiple model passes (policy + reference + reward + value models).</li> <li>Fine-tuning large models like GPT-3 with PPO can take thousands of GPU-hours.</li> </ul>"},{"location":"references/#3-reward-hacking","title":"\u26a0\ufe0f 3. Reward Hacking","text":"<p>PPO optimizes for the reward model \u2014 not true human satisfaction. If the reward model is imperfect, the LLM can \u201cgame\u201d it by producing text that tricks it (e.g., overly polite or verbose responses).</p> <p>This is called reward model over-optimization or \u201creward hacking.\u201d</p>"},{"location":"references/#4-sparse-or-noisy-rewards","title":"\ud83e\uddee 4. Sparse or Noisy Rewards","text":"<ul> <li>Human preference data can be subjective or inconsistent.  </li> <li>If rewards are too sparse (few signals per episode), PPO struggles to learn efficiently.</li> </ul>"},{"location":"references/#5-credit-assignment-problem","title":"\ud83d\udd01 5. Credit Assignment Problem","text":"<p>PPO updates are computed per-token, but rewards are given per-sequence. It\u2019s not always clear which token or phrase made the answer better or worse.</p>"},{"location":"references/#6-trade-off-between-exploration-and-alignment","title":"\u2696\ufe0f 6. Trade-off Between Exploration and Alignment","text":"<p>PPO encourages exploration (trying new actions) but in LLMs, that may generate toxic or unsafe text. Balancing alignment with diversity remains difficult.</p>"},{"location":"references/#7-implementation-complexity","title":"\ud83d\udd0d 7. Implementation Complexity","text":"<ul> <li>Requires maintaining multiple models in memory (policy, reward, value, reference).  </li> <li>Needs careful hyperparameter tuning (clip range, learning rate, KL coeff., etc.).  </li> <li>Often unstable if any component is suboptimal.</li> </ul>"},{"location":"references/#9-summary","title":"9. Summary","text":"Component Role Example Policy Model (LLM) Generates responses to prompts <code>GPT-3</code>, <code>Llama-2</code> Reward Model Scores how much humans like the output Fine-tuned classifier PPO Algorithm Updates the policy using rewards Training loop KL Penalty Prevents over-deviation from base model Regularization Goal Align LLM behavior with human intent Helpful, safe, and truthful answers"},{"location":"references/#10-alternatives-and-evolutions","title":"10. Alternatives and Evolutions","text":"<p>To address PPO\u2019s complexity, several improved methods have emerged: - DPO (Direct Preference Optimization) \u2014 bypasses reward models and RL by directly optimizing log-probabilities from pairwise preferences. - DRPO (Deterministic Regularized PPO) \u2014 a more stable PPO variant that removes stochasticity in updates. - GRPO (Generalized PPO) \u2014 extends PPO to handle richer feedback (e.g., multiple objectives, multi-turn context). - KTO (Kullback-Leibler Trust Optimization) \u2014 simplifies the KL constraint using direct divergence minimization.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/","title":"DeepSeek-R1: Reinforcement Learning for Fine-Tuning LLMs","text":"<p>This document provides a detailed overview of the DeepSeek-R1 strategy for fine-tuning and preference tuning large language models (LLMs). It covers the RL methods, distinctions from traditional approaches, GRPO optimization, multi-stage training pipeline, reward design, and model distillation.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#overview","title":"Overview","text":"<p>DeepSeek-R1 introduces a novel approach to improving reasoning capabilities and general instruction-following in LLMs using reinforcement learning (RL). Unlike traditional RLHF methods, DeepSeek focuses on verifiable tasks and multi-stage training pipelines to efficiently align models for reasoning and preference tasks.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#key-variants","title":"Key Variants","text":"<ul> <li>DeepSeek-R1-Zero: RL-only variant without initial supervised fine-tuning (SFT). Uses verifiable reasoning tasks (math, code, logic) as reward signals.</li> <li>DeepSeek-R1: Multi-stage pipeline starting with cold-start SFT, followed by reasoning-oriented RL, dataset generation, SFT fine-tuning, and a second RL stage for broader instruction-following.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#grpo-group-relative-policy-optimization","title":"GRPO (Group Relative Policy Optimization)","text":"<p>GRPO is the core RL optimization algorithm used in DeepSeek-R1. It adapts traditional PPO to work with multiple outputs per prompt and relative ranking of outputs.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#key-features","title":"Key Features","text":"<ul> <li>Group-based ranking: For each prompt (q), sample a group of outputs ({o_i}_{i=1}^G) from the current policy.</li> <li>Advantage computation: Compute an advantage score (A_i) for each output.</li> <li>Clipped-ratio objective: Optimizes the policy using a PPO-style clipped ratio but applied to each output in the group, using relative ranks rather than absolute rewards.</li> <li>KL penalty: Ensures the new policy does not deviate significantly from a reference policy.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#grpo-objective-function-simplified","title":"GRPO Objective Function (simplified)","text":"<pre><code>J_GRPO(\u03b8) = E_{q,{o_i}} [ (1/G) \u2211_i min( r_i * A_i, clip(r_i, 1-\u03b5, 1+\u03b5) * A_i ) ] - \u03b2 * KL(\u03c0_\u03b8 || \u03c0_ref)\nwhere r_i = \u03c0_\u03b8(o_i|q) / \u03c0_{\u03b8_old}(o_i|q)\n</code></pre> <p>This ensures stable policy updates while leveraging multiple candidate outputs per prompt.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#reward-design","title":"Reward Design","text":""},{"location":"advanced_topics/deepseek_rl_finetuning/#reasoning-oriented-tasks","title":"Reasoning-Oriented Tasks","text":"<ul> <li>Correctness: Verified automatically via solvers or compilers.</li> <li>Language Consistency: Penalizes multi-language outputs and encourages coherent chain-of-thought.</li> <li>Weighted sum: Overall reward combines correctness and readability/style.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#general-instruction-following-tasks","title":"General Instruction-Following Tasks","text":"<ul> <li>Preference models: Mixture of rule-based checks and learned reward models for helpfulness, harmlessness, and style alignment.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#multi-stage-training-pipeline","title":"Multi-Stage Training Pipeline","text":"<ol> <li>Cold-start SFT: Small curated dataset of chain-of-thought (CoT) reasoning examples.</li> <li>Reasoning-oriented RL: GRPO algorithm applied with verifiable reward signals.</li> <li>Rejection Sampling \u2192 SFT Dataset: Filter RL outputs for quality and readability, then fine-tune using SFT.</li> <li>Second RL Stage (All Scenarios): Broader prompt coverage for general instruction-following using mixed reward signals.</li> <li>Distillation to Smaller Models: Generate dataset from large RL-trained model and fine-tune smaller models via SFT to inherit reasoning capability.</li> </ol>"},{"location":"advanced_topics/deepseek_rl_finetuning/#distinctions-from-traditional-methods","title":"Distinctions from Traditional Methods","text":"Feature Conventional RLHF / SFT + RL DeepSeek-R1 Strategy Initial SFT Large human-annotated dataset R1-Zero: None; R1: small cold-start SFT Reward Source Learned reward model from human preference Reasoning: rule-based correctness + ranking; General: mixture Policy Optimization PPO using reward model GRPO (group ranking + clipped ratio + KL penalty) Domain Focus Broad instruction-following Emphasis on reasoning \u2192 general instructions Post-RL Dataset Generation Sometimes RL outputs \u2192 filter \u2192 SFT dataset Distillation to Smaller Models Optional Explicit large \u2192 dataset \u2192 smaller SFT Emergence of Reasoning Through CoT SFT + RLHF Emergent via RL alone (R1-Zero)"},{"location":"advanced_topics/deepseek_rl_finetuning/#advantages","title":"Advantages","text":"<ul> <li>Emergent reasoning via RL alone.</li> <li>Efficient multi-stage training combining SFT and RL.</li> <li>Verifiable rewards reduce noise and instability.</li> <li>GRPO allows stable optimization across multiple outputs.</li> <li>Distillation enables smaller, deployable models without expensive RL training.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#limitations","title":"Limitations","text":"<ul> <li>Transparency of datasets and hyperparameters is limited.</li> <li>Generalization to arbitrary instructions may still be challenging.</li> <li>R1-Zero may have readability and language-mixing issues.</li> <li>Smaller distilled models rely solely on SFT and may not fully retain RL benefits.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#references","title":"References","text":"<ul> <li>DeepSeek-R1 Paper Summary</li> <li>GRPO Explanation</li> <li>DeepSeek-R1 Analysis</li> <li>Training Pipeline &amp; Distillation</li> <li>DeepSeek-R1 Limitations</li> </ul>"},{"location":"methods/dpo/","title":"\ud83e\udde9 Direct Preference Optimization (DPO) \u2014 Simplified Overview","text":""},{"location":"methods/dpo/#what-is-dpo","title":"\ud83e\udde0 What Is DPO?","text":"<p>Direct Preference Optimization (DPO) is a reinforcement learning-free method to fine-tune large language models (LLMs) using human feedback. Instead of using a reward model and reinforcement learning (like PPO does), DPO directly learns from pairs of responses \u2014 one \u201cpreferred\u201d and one \u201crejected\u201d \u2014 for the same prompt.</p>"},{"location":"methods/dpo/#how-it-works","title":"\ud83d\ude80 How It Works","text":""},{"location":"methods/dpo/#1-training-inputs","title":"1. Training Inputs","text":"<p>Each training example consists of:</p> <pre><code>(prompt, chosen_answer, rejected_answer)\n</code></pre> <ul> <li>Prompt (x): A user query (e.g., \u201cExplain quantum computing simply.\u201d)</li> <li>Chosen Answer (y\u207a): The preferred (good) response.</li> <li>Rejected Answer (y\u207b): The less preferred (bad) response.</li> </ul>"},{"location":"methods/dpo/#2-training-objective","title":"2. Training Objective","text":"<p>The DPO policy model (\u03c0\u209c) learns to increase the likelihood of generating the chosen answer and decrease the likelihood of generating the rejected one \u2014 while staying close to a reference model (\u03c0_ref), which is usually the base model before fine-tuning.</p> <p>Unlike PPO, there\u2019s no reward model or KL penalty tuning loop. DPO directly optimizes a closed-form objective derived from the reward preference formulation.</p>"},{"location":"methods/dpo/#training-vs-inference","title":"\u2699\ufe0f Training vs Inference","text":"Phase What Happens Models Involved Training The model sees pairs of responses and learns to prefer the \u201cgood\u201d one. Policy model (trainable) + Reference model (frozen) Inference The trained model generates responses directly \u2014 no reference or reward model needed. Only the fine-tuned policy model"},{"location":"methods/dpo/#key-takeaway","title":"\u2705 Key Takeaway","text":"<p>At inference time, DPO-trained models behave like normal LLMs. They \u201cremember\u201d what good answers look like \u2014 so no additional machinery (like reward models or RL loops) is needed.</p>"},{"location":"methods/dpo/#analogy","title":"\ud83e\udde9 Analogy","text":"<p>Think of teaching a student to write essays.</p> <ul> <li>PPO Way: The student writes essays, gets a numeric grade (from a reward model), and iteratively improves.  </li> <li>DPO Way: The student is shown two essays for the same topic \u2014 one good, one bad \u2014 and learns which one is better.</li> </ul> <p>After training, the DPO student doesn\u2019t need feedback or grades anymore \u2014 they\u2019ve internalized what \u201cgood\u201d writing looks like.</p>"},{"location":"methods/dpo/#comparison-dpo-vs-ppo","title":"\u2696\ufe0f Comparison: DPO vs PPO","text":"Feature PPO DPO Uses Reward Model? \u2705 Yes \u274c No Needs RL Optimization Loop? \u2705 Yes \u274c No Training Stability Can be unstable (KL tuning required) More stable Implementation Complexity High (policy gradient, reward normalization, etc.) Simple (log-likelihood comparison) Scalability Expensive \u2014 requires rollout generation Lightweight and efficient Interpretability Less direct (reward shaping needed) More transparent \u2014 uses preference data directly"},{"location":"methods/dpo/#limitations-caveats","title":"\u26a0\ufe0f Limitations &amp; Caveats","text":"<ol> <li> <p>Limited Labeled Data:    DPO relies on datasets where pairs of responses (good vs. bad) are available. Collecting large, high-quality preference data is expensive.</p> </li> <li> <p>Generalization Challenge:    The model only learns preferences within the data distribution it was trained on \u2014 performance may drop for unseen domains.</p> </li> <li> <p>Reference Model Dependency:    The choice of the reference model (\u03c0_ref) affects performance. A poor reference can make optimization harder.</p> </li> <li> <p>No Reward Signal for Exploration:    Without a continuous reward, DPO may not explore diverse or novel answers as effectively as PPO.</p> </li> <li> <p>Preference Noise:    If human feedback is inconsistent or noisy, DPO can amplify that bias.</p> </li> </ol>"},{"location":"methods/dpo/#inference-usage","title":"\ud83e\uddea Inference Usage","text":"<p>Once the DPO model is trained, it\u2019s used just like any normal LLM:</p> <pre><code>User Prompt \u2192 DPO-trained model \u2192 Response\n</code></pre> <p>There\u2019s no need for a reference or reward model at this stage \u2014 all learning has been baked into the weights during training.</p>"},{"location":"methods/dpo/#summary","title":"\ud83d\udca1 Summary","text":"<ul> <li>DPO is a direct and efficient way to align LLMs with human preferences.  </li> <li>It avoids reinforcement learning while still achieving preference alignment.  </li> <li>During inference, the model behaves like a standard LLM \u2014 just better aligned.</li> </ul>"},{"location":"methods/dpo/#optional-code-skeleton-hugging-face-trl","title":"\ud83e\uddf0 Optional Code Skeleton (Hugging Face TRL)","text":"<pre><code>from trl import DPOTrainer\n\ntrainer = DPOTrainer(\n    model=policy_model,\n    ref_model=reference_model,\n    args=training_args,\n    train_dataset=preference_dataset,\n)\n\ntrainer.train()\n</code></pre>"},{"location":"methods/dpo/#references","title":"\ud83d\udcd8 References","text":"<ul> <li>\u201cDirect Preference Optimization: Your Language Model is Secretly a Reward Model\u201d \u2014 Rafailov et al., 2023  </li> <li>Hugging Face TRL: https://huggingface.co/docs/trl</li> <li>Blog: \u201cSimplifying RLHF with DPO\u201d \u2014 Hugging Face, 2023</li> </ul>"},{"location":"methods/drpo/","title":"DRPO (Direct Reward Policy Optimization) for LLM Fine-Tuning","text":"<p>This document provides a detailed overview of DRPO, a method for fine-tuning large language models (LLMs) using reward-based alignment. It also compares DRPO with other approaches like PPO and DPO, highlighting workflow, advantages, limitations, and performance considerations.</p>"},{"location":"methods/drpo/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Key Idea</li> <li>Training Workflow</li> <li>Where Rewards Come From</li> <li>Differences from PPO</li> <li>Differences from DPO</li> <li>Advantages of DRPO</li> <li>Limitations / Issues</li> <li>Performance Comparison</li> <li>When to Use DRPO</li> <li>Workflow Diagram</li> </ul>"},{"location":"methods/drpo/#overview","title":"Overview","text":"<p>Direct Reward Policy Optimization (DRPO) is an offline, reward-weighted fine-tuning method for LLMs. It is inspired by reinforcement learning techniques like PPO but avoids the expensive and unstable online RL loop.</p> <p>DRPO is used to align LLMs to produce helpful, safe, and high-quality responses based on feedback captured in a reward model (RM).</p>"},{"location":"methods/drpo/#key-idea","title":"Key Idea","text":"<ul> <li>Start with a base model (often SFT-fine-tuned).  </li> <li>Generate multiple responses per prompt using this base model.  </li> <li>Use a reward model to assign a scalar reward to each response.  </li> <li>Fine-tune the model using a weighted log-likelihood loss, encouraging high-reward responses while applying a KL penalty to maintain closeness to the base model.</li> </ul> <p>DRPO Loss Function:</p> <p>[ L(\\pi) = - \\mathbb{E}_{(x, y)} [ r(x, y) \\cdot \\log \\pi(y|x) ] + \\beta KL(\\pi || \\pi_0) ]</p> <p>Where: - (r(x, y)) = reward score from the reward model - (\\pi) = fine-tuned model - (\\pi_0) = base (SFT) model - (\\beta) = KL coefficient</p>"},{"location":"methods/drpo/#training-workflow","title":"Training Workflow","text":""},{"location":"methods/drpo/#drpo-offline","title":"DRPO (Offline)","text":"<ol> <li>Generate a fixed dataset of (prompt, response) pairs from the SFT model.  </li> <li>Compute reward scores for all responses using the reward model.  </li> <li>Train the LLM with weighted log-likelihood, using the reward as weights and a KL penalty.  </li> <li>No iterative online rollouts are needed.</li> </ol>"},{"location":"methods/drpo/#ppo-online-rlhf","title":"PPO (Online RLHF)","text":"<ol> <li>Initialize policy with SFT model.  </li> <li>Generate responses online with the current policy.  </li> <li>Score responses using the reward model.  </li> <li>Compute advantages and perform policy gradient updates.  </li> <li>Repeat the loop for many iterations \u2014 the policy evolves, generating new responses each step.</li> </ol> <p>Key Difference: DRPO uses offline, precomputed data, while PPO relies on online, on-policy exploration.</p>"},{"location":"methods/drpo/#where-rewards-come-from","title":"Where Rewards Come From","text":"<ul> <li>Rewards are computed from a reward model trained on human preference data.  </li> <li>Candidate responses are generated by the SFT model.  </li> <li>The reward model assigns a scalar value for each (prompt, response) pair, which DRPO uses as weights during training.</li> </ul> <p>Dataset Example:</p> Prompt Response Reward \"Explain photosynthesis.\" \"Plants use sunlight to make energy...\" 0.92 \"Explain photosynthesis.\" \"It\u2019s about light and plants.\" 0.45"},{"location":"methods/drpo/#differences-from-ppo","title":"Differences from PPO","text":"Aspect PPO DRPO Training loop Online, iterative RL Offline, single-pass fine-tuning Reward usage Used online for policy gradient Used offline as weighting for supervised loss Exploration Strong \u2014 policy generates new outputs each step Limited \u2014 only SFT-generated responses Stability Can be unstable Stable like supervised learning Training cost High (many rollouts, multiple gradient steps) Low (single dataset pass) Performance Slightly better alignment Close to PPO but slightly lower"},{"location":"methods/drpo/#differences-from-dpo","title":"Differences from DPO","text":"Aspect DPO DRPO Input data Preference pairs (chosen vs rejected) Scalar reward values Loss Logistic loss on pairwise comparisons Weighted log-likelihood on reward Feedback Human preferences Reward model scores Behavior Learns which output is better Learns how much better an output is"},{"location":"methods/drpo/#advantages-of-drpo","title":"Advantages of DRPO","text":"<ul> <li>Stable training: Avoids PPO\u2019s gradient instability.  </li> <li>Faster &amp; cheaper: Single-pass offline training.  </li> <li>Simple implementation: Just a modified supervised fine-tuning loss.  </li> <li>Good alignment: Reward-aware fine-tuning can improve response quality efficiently.</li> </ul>"},{"location":"methods/drpo/#limitations-issues","title":"Limitations / Issues","text":"<ul> <li>Reward model bias: DRPO inherits any biases in the reward model.  </li> <li>Limited exploration: Only improves on existing SFT responses.  </li> <li>Reward overfitting: Risk of \u201creward hacking\u201d if the reward model is not robust.  </li> <li>KL tuning required: Poor KL weight can lead to underfitting or overfitting.  </li> <li>Data requirement: Needs a sufficiently diverse set of reward-scored responses.</li> </ul>"},{"location":"methods/drpo/#performance-comparison","title":"Performance Comparison","text":"Metric PPO DRPO Alignment quality Slightly higher Slightly lower, ~90\u201395% of PPO Training stability Can be unstable Very stable Training cost Very high 5\u201320\u00d7 faster Exploration Strong Limited Practical use Final alignment pass in large LLMs Efficient intermediate alignment"},{"location":"methods/drpo/#when-to-use-drpo","title":"When to Use DRPO","text":"<ul> <li>You already have a trained reward model.  </li> <li>You want fast, stable, and cost-effective alignment.  </li> <li>You want to fine-tune using existing SFT responses rather than running expensive online RL loops.  </li> <li>Useful for intermediate-stage alignment before deploying PPO or other full RL methods.</li> </ul>"},{"location":"methods/drpo/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>      SFT Model (\u03c0\u2080)\n           |\n           | Generate responses (offline or online)\n           v\n     Candidate Responses\n           |\n           | Score with Reward Model (R)\n           v\n   ------------------------\n   |                      |\nDRPO (Offline)        PPO (Online)\nWeighted Log-Likelihood  Policy Gradient Updates\n+ KL Penalty             + KL Penalty\nSingle-pass training      Iterative rollouts &amp; training\nLimited exploration       Strong exploration\nStable &amp; fast             Slower, high cost, slightly higher alignment\n</code></pre>"},{"location":"methods/drpo/#references","title":"References","text":"<ul> <li>Christiano, P., et al. (2017). Deep Reinforcement Learning from Human Preferences. </li> <li>Bai, Y., et al. (2022). Training a Helpful and Harmless Assistant with RLHF. </li> <li>Stiennon, N., et al. (2022). Learning to summarize with human feedback.</li> </ul>"},{"location":"methods/g%28uided%29rpo/","title":"GRPO: Guided Reward Policy Optimization for LLM Fine-Tuning","text":"<p>GRPO (Guided Reward Policy Optimization) is an RL-based approach for fine-tuning Large Language Models (LLMs), emphasizing structured or \u201cguided\u201d reward signals to improve instruction-following, safety, and multi-step reasoning capabilities.</p>"},{"location":"methods/g%28uided%29rpo/#1-what-is-grpo","title":"1. What is GRPO?","text":"<p>GRPO is a method to fine-tune LLMs using guided rewards instead of relying solely on scalar rewards or pairwise preference comparisons. The guiding signals can come from:</p> <ul> <li>Human feedback (e.g., good/bad labels or stepwise evaluations)</li> <li>Smaller LLMs generating intermediate feedback</li> <li>Programmatic or heuristic scoring functions</li> </ul> <p>The core idea is to provide richer, structured guidance so the model learns efficiently, avoids reward hacking, and handles complex multi-step tasks.</p>"},{"location":"methods/g%28uided%29rpo/#2-how-grpo-works-step-by-step","title":"2. How GRPO Works (Step-by-Step)","text":"<ol> <li> <p>Start with a Pretrained LLM    The base model has general knowledge but may not follow instructions optimally.</p> </li> <li> <p>Collect Feedback / Guidance </p> </li> <li>Can be human-annotated, LLM-generated, or heuristic-based.  </li> <li> <p>May include stepwise intermediate rewards, not just a final score.</p> </li> <li> <p>Policy Optimization </p> </li> <li>Treat the LLM as a policy: <code>input -&gt; response -&gt; guided reward</code>.  </li> <li> <p>Update the model to maximize expected guided reward, optionally including KL divergence to the base model.</p> </li> <li> <p>Iterate </p> </li> <li> <p>Repeat feedback \u2192 policy update \u2192 evaluation until convergence.</p> </li> <li> <p>Optional Safety Mechanisms </p> </li> <li>KL penalty to prevent the model from drifting too far from the base model.</li> </ol>"},{"location":"methods/g%28uided%29rpo/#3-reward-signals-in-grpo","title":"3. Reward Signals in GRPO","text":"<ul> <li>Flexible: Can use human annotations, LLM feedback, heuristics, or combinations.  </li> <li>Structured: Can be stepwise or multi-dimensional, tracking reasoning steps, factual correctness, coherence, safety, or style.  </li> <li>Adaptive: Guided reward signals reduce sparse or ambiguous reward issues present in standard RL methods.</li> </ul>"},{"location":"methods/g%28uided%29rpo/#4-kl-penalty-in-grpo","title":"4. KL Penalty in GRPO","text":"<ul> <li>Purpose: Ensure fine-tuned model doesn\u2019t drift too far from the base model, maintaining fluency, factual grounding, and stability.  </li> <li>Implementation: </li> <li>Often adaptive or applied per intermediate step.  </li> <li>Can be soft or measured via embedding similarity.  </li> </ul> <p>Objective function example:</p> <p>[ \\max_\\pi \\; \\mathbb{E}{x \\sim D} \\Big[ R{\\text{guided}}(x, \\pi(x)) - \\beta \\, \\text{KL}(\\pi(\\cdot|x) \\parallel \\pi_{\\text{ref}}(\\cdot|x)) \\Big] ]</p> <p>Where: - ( R_{\\text{guided}} ) = guided reward (human + LLM + heuristics) - ( \\pi_{\\text{ref}} ) = base model policy - ( \\beta ) = KL regularization coefficient  </p>"},{"location":"methods/g%28uided%29rpo/#5-advantages-of-grpo","title":"5. Advantages of GRPO","text":"<ul> <li>Faster convergence with fewer samples.  </li> <li>Reduced risk of reward hacking.  </li> <li>Flexible and expressive reward shaping (multi-step, hierarchical, multi-objective).  </li> <li>Handles complex instruction-following, multi-step reasoning, and safety-critical tasks.  </li> </ul>"},{"location":"methods/g%28uided%29rpo/#6-limitations-things-to-watch","title":"6. Limitations / Things to Watch","text":"<ul> <li>Requires high-quality guided rewards; noisy or biased signals can harm performance.  </li> <li>Computationally heavier than simple preference-based methods.  </li> <li>Designing stepwise or structured rewards requires careful planning.  </li> <li>Less standardized tooling compared to PPO or DRPO.</li> </ul>"},{"location":"methods/g%28uided%29rpo/#7-comparison-with-ppo-and-drpo","title":"7. Comparison with PPO and DRPO","text":"Method Reward Type Offline/Online KL Penalty Notes PPO Scalar reward (learned RM) Online RLHF \u2705 Yes, explicit Stable, widely used, but risk of reward hacking DRPO Pairwise preference Offline \u274c No Simple preference-based offline method GRPO Guided / shaped reward (scalar, stepwise, vector) Offline or online \u2705 Yes, adaptive/local More expressive and structured rewards, reduces reward hacking"},{"location":"methods/g%28uided%29rpo/#8-example-use-cases","title":"8. Example Use Cases","text":"<ol> <li>Multi-step reasoning tasks: math problems, coding, multi-step planning.  </li> <li>Safety-critical tasks: medical advice, legal guidance, regulated instructions.  </li> <li>Long-horizon tasks: summarization of long documents, multi-turn dialogues.  </li> <li>Complex instruction-following: recipes, multi-step API workflows, simulations.  </li> </ol>"},{"location":"methods/g%28uided%29rpo/#9-relationship-to-drpo","title":"9. Relationship to DRPO","text":"<ul> <li>GRPO can be seen as an extension of DRPO, both are offline methods.  </li> <li>DRPO uses simple preference comparisons.  </li> <li>GRPO adds explicit, structured, and guided reward signals, including stepwise feedback, making learning more efficient and controlled.</li> </ul>"},{"location":"methods/g%28uided%29rpo/#10-summary","title":"10. Summary","text":"<p>GRPO is like DRPO with a smarter teacher: instead of just saying \u201cbetter or worse,\u201d it guides the model at each step with informative feedback. The KL penalty ensures the model remains grounded in the base model\u2019s knowledge, while guided rewards accelerate learning and reduce reward hacking.  </p>"},{"location":"methods/ppo/","title":"\ud83e\udde0 PPO and Reward Models in LLM Training","text":""},{"location":"methods/ppo/#1-overview","title":"1. Overview","text":"<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm widely used in fine-tuning Large Language Models (LLMs) under the Reinforcement Learning from Human Feedback (RLHF) framework. It helps bridge the gap between human preferences and LLM outputs by optimizing the model\u2019s responses to align with what humans find helpful, safe, or relevant.</p>"},{"location":"methods/ppo/#2-the-big-picture-rlhf-pipeline","title":"2. The Big Picture: RLHF Pipeline","text":"<p>RLHF typically follows three key stages:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT)    Train a base LLM on high-quality human demonstration data (prompt\u2013response pairs).</p> </li> <li> <p>Reward Model (RM) Training    Train a separate model to predict a scalar reward from human preference data. For example, given two responses to the same prompt, the model learns which one humans prefer.</p> </li> <li> <p>Reinforcement Learning (e.g., PPO)    Use PPO to fine-tune the SFT model (the \u201cpolicy\u201d) so that it generates outputs that maximize the predicted reward given by the RM.</p> </li> </ol>"},{"location":"methods/ppo/#3-intuitive-understanding-of-ppo","title":"3. Intuitive Understanding of PPO","text":"<p>Imagine you have an assistant (the LLM) that gives answers. You ask multiple questions, and a teacher (the reward model) scores how good each answer is. PPO helps you adjust the assistant\u2019s behavior to get higher scores without deviating too far from its original style (the SFT model).</p> <p>The \u201cproximal\u201d in PPO means:  </p> <p>\u201cDon\u2019t change the model\u2019s behavior too drastically in one update.\u201d</p> <p>This stability comes from a clipped objective function that penalizes large policy updates.</p>"},{"location":"methods/ppo/#4-ppo-in-simple-terms","title":"4. PPO in Simple Terms","text":"<ol> <li> <p>Policy model (the LLM):    Generates a response (sequence of tokens) given a prompt.</p> </li> <li> <p>Reward model:    Assigns a reward (score) to the response.</p> </li> <li> <p>Old policy (reference model):    A frozen snapshot of the model before PPO updates \u2014 used to measure how much the new model diverges.</p> </li> <li> <p>PPO update: </p> </li> <li>Compare the new model\u2019s likelihood of the generated tokens to the old model\u2019s likelihood.  </li> <li>Compute the reward advantage (how much better the new behavior is).  </li> <li>Clip the ratio of new/old probabilities to prevent large steps.</li> </ol>"},{"location":"methods/ppo/#5-ppo-objective-function","title":"5. PPO Objective Function","text":"<p>[ L^{PPO}(\\theta) = \\mathbb{E}_t \\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right] ]</p> <p>Where: - (r_t(\\theta)) = ratio between new and old policy probabilities. - (A_t) = advantage function (how much better an action is than expected). - (\\epsilon) = clipping threshold (often 0.1\u20130.3).</p> <p>This ensures small but stable improvements.</p>"},{"location":"methods/ppo/#6-example-of-ppo-in-practice","title":"6. Example of PPO in Practice","text":"<pre><code>for prompt in prompts:\n    # 1. Generate response\n    response = policy_model.generate(prompt)\n\n    # 2. Compute reward from reward model\n    reward = reward_model(prompt, response)\n\n    # 3. Compute log probabilities from old and new policies\n    logprobs_old = ref_model.logprobs(prompt, response)\n    logprobs_new = policy_model.logprobs(prompt, response)\n\n    # 4. Compute advantage and clipped objective\n    ratio = torch.exp(logprobs_new - logprobs_old)\n    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n    loss = -torch.mean(torch.min(ratio * advantage, clipped_ratio * advantage))\n\n    # 5. Backpropagate and update model\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"methods/ppo/#7-why-ppo-instead-of-direct-human-feedback","title":"7. Why PPO Instead of Direct Human Feedback?","text":"<p>Because human annotation is limited and noisy: - We can\u2019t rate every output of a large model. - Direct training on human labels doesn\u2019t handle the credit assignment problem \u2014 which part of the response caused the high or low rating? - PPO allows the model to learn generalizable feedback signals through the reward model, without human labels for every step.</p>"},{"location":"methods/ppo/#8-limitations-and-challenges-of-ppo-in-llm-training","title":"8. Limitations and Challenges of PPO in LLM Training","text":""},{"location":"methods/ppo/#1-kl-divergence-penalty-instability","title":"\ud83e\udde9 1. KL Divergence Penalty Instability","text":"<p>To ensure the fine-tuned model doesn\u2019t drift too far from the base model, PPO adds a KL penalty: [ L = L^{PPO} - \\beta D_{KL}(\\pi_{\\theta} || \\pi_{\\text{ref}}) ] However: - Choosing the right KL coefficient ((\\beta)) is tricky. - Too small \u2192 model diverges, harming coherence/safety. - Too large \u2192 model learns too little (under-updates).</p> <p>Modern techniques (e.g., adaptive KL control) try to auto-adjust this balance.</p>"},{"location":"methods/ppo/#2-high-training-time-and-cost","title":"\u23f3 2. High Training Time and Cost","text":"<ul> <li>PPO fine-tuning is computationally expensive, requiring multiple model passes (policy + reference + reward + value models).</li> <li>Fine-tuning large models like GPT-3 with PPO can take thousands of GPU-hours.</li> </ul>"},{"location":"methods/ppo/#3-reward-hacking","title":"\u26a0\ufe0f 3. Reward Hacking","text":"<p>PPO optimizes for the reward model \u2014 not true human satisfaction. If the reward model is imperfect, the LLM can \u201cgame\u201d it by producing text that tricks it (e.g., overly polite or verbose responses).</p> <p>This is called reward model over-optimization or \u201creward hacking.\u201d</p>"},{"location":"methods/ppo/#4-sparse-or-noisy-rewards","title":"\ud83e\uddee 4. Sparse or Noisy Rewards","text":"<ul> <li>Human preference data can be subjective or inconsistent.  </li> <li>If rewards are too sparse (few signals per episode), PPO struggles to learn efficiently.</li> </ul>"},{"location":"methods/ppo/#5-credit-assignment-problem","title":"\ud83d\udd01 5. Credit Assignment Problem","text":"<p>PPO updates are computed per-token, but rewards are given per-sequence. It\u2019s not always clear which token or phrase made the answer better or worse.</p>"},{"location":"methods/ppo/#6-trade-off-between-exploration-and-alignment","title":"\u2696\ufe0f 6. Trade-off Between Exploration and Alignment","text":"<p>PPO encourages exploration (trying new actions) but in LLMs, that may generate toxic or unsafe text. Balancing alignment with diversity remains difficult.</p>"},{"location":"methods/ppo/#7-implementation-complexity","title":"\ud83d\udd0d 7. Implementation Complexity","text":"<ul> <li>Requires maintaining multiple models in memory (policy, reward, value, reference).  </li> <li>Needs careful hyperparameter tuning (clip range, learning rate, KL coeff., etc.).  </li> <li>Often unstable if any component is suboptimal.</li> </ul>"},{"location":"methods/ppo/#9-summary","title":"9. Summary","text":"Component Role Example Policy Model (LLM) Generates responses to prompts <code>GPT-3</code>, <code>Llama-2</code> Reward Model Scores how much humans like the output Fine-tuned classifier PPO Algorithm Updates the policy using rewards Training loop KL Penalty Prevents over-deviation from base model Regularization Goal Align LLM behavior with human intent Helpful, safe, and truthful answers"},{"location":"methods/ppo/#10-alternatives-and-evolutions","title":"10. Alternatives and Evolutions","text":"<p>To address PPO\u2019s complexity, several improved methods have emerged: - DPO (Direct Preference Optimization) \u2014 bypasses reward models and RL by directly optimizing log-probabilities from pairwise preferences. - DRPO (Deterministic Regularized PPO) \u2014 a more stable PPO variant that removes stochasticity in updates. - GRPO (Generalized PPO) \u2014 extends PPO to handle richer feedback (e.g., multiple objectives, multi-turn context). - KTO (Kullback-Leibler Trust Optimization) \u2014 simplifies the KL constraint using direct divergence minimization.</p>"}]}