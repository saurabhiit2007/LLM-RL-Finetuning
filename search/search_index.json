{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning for Fine-Tuning Large Language Models","text":"<p>Welcome to the repository on RL-based fine-tuning of LLMs. This project explores methods for aligning large language models with human preferences, improving instruction following, and optimizing task-specific performance.</p>"},{"location":"#why-rl-based-fine-tuning","title":"Why RL-Based Fine-Tuning?","text":"<p>Large Language Models (LLMs) trained on massive text corpora often produce outputs that are plausible but not always aligned with user intent or desired behavior. Reinforcement Learning (RL) enables fine-tuning LLMs by optimizing for specific objectives, human feedback, or pairwise preferences, allowing models to generate more reliable and controllable responses.</p>"},{"location":"#methods-overview","title":"Methods Overview","text":"<p>This repository covers the following key RL-based fine-tuning methods:</p> Method Focus Highlights PPO Proximal Policy Optimization Balances exploration and stability in policy updates. DPO Direct Preference Optimization Aligns model outputs with human preferences without a full RL loop. DRPO Decoupled Reward Policy Optimization Separates reward modeling from policy updates for improved stability. GRPO Guided Relative Policy Optimization Incorporates pairwise comparisons or structured guidance during training. Direct Reward Policy Optimization (DRPO-Offline) Offline Reward Optimization Uses pre-collected reward data to guide policy updates without live RL interaction. Guided RPO Guided Reward Policy Optimization Combines structured guidance with pairwise preference learning for improved alignment. <p>Each method page contains detailed explanations, mathematical formulations, and references for further reading.</p>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<p>In addition to the main methods, the repository also explores advanced topics related to RL fine-tuning:</p> <ul> <li>KL Penalty in Policy Optimization \u2013 Techniques to stabilize learning by penalizing deviations from reference policies.  </li> <li>Reward Hacking \u2013 Understanding how misaligned rewards can lead to unintended behaviors.  </li> <li>DeepSeek RL Details \u2013 In-depth insights into the DeepSeek approach to LLM alignment.</li> </ul>"},{"location":"references/","title":"\ud83d\udcda References","text":""},{"location":"references/#1-ppo-proximal-policy-optimization","title":"\ud83e\uddee 1. PPO - Proximal Policy Optimization","text":"<ul> <li>Schulman et al. (2017) \u2014 Proximal Policy Optimization Algorithms. [arXiv:1707.06347]</li> <li>Adaptive-ML Blog (2023) \u2014 From Zero to PPO: Understanding the Path to Helpful AI Models. [Link]</li> <li>Secrets of RLHF in LLMs (2023) \u2014 Part I: PPO Explained in Detail. [arXiv:2307.04964]</li> </ul>"},{"location":"references/#2-dpo-direct-preference-optimization","title":"\ud83c\udfaf 2. DPO - Direct Preference Optimization","text":"<ul> <li>Rafailov et al. (2023) \u2014 Direct Preference Optimization: Your Language Model is Secretly a Reward Model. [arXiv:2305.18290]</li> <li>Hugging Face Blog (2023) \u2014 Simplifying RLHF with DPO. [Blog]</li> <li>Implementation Repo \u2014 eric-mitchell/direct-preference-optimization</li> </ul>"},{"location":"references/#3-grpo-grouped-relative-policy-optimization","title":"\ud83d\udd01 3. GRPO - Grouped Relative Policy Optimization","text":"<ul> <li>Shao et al. (2024) \u2014 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. (Introduces GRPO) [arXiv:2402.03300]</li> <li>Mroueh et al. (2025) \u2014 Revisiting Group Relative Policy Optimization. [arXiv:2505.22257]</li> <li>Samia Sahin (2025) \u2014 The Math Behind DeepSeek \u2014 GRPO Explained. [Medium]</li> </ul>"},{"location":"references/#3-drpo-decoupled-rewards-policy-optimization","title":"\ud83d\udd01 3. DRPO - Decoupled Rewards Policy Optimization","text":"<ul> <li>Li et al. (2025) - DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization. DRPO Paper</li> </ul>"},{"location":"references/#4-kl-penalty","title":"\u2696\ufe0f 4. KL Penalty","text":"<ul> <li>APXML Guide (2023) \u2014 KL Divergence Penalty in RLHF. [Article]</li> </ul>"},{"location":"references/#5-deepseek-rl-reinforcement-learning-for-reasoning","title":"\ud83d\ude80 5. DeepSeek RL \u2014 Reinforcement Learning for Reasoning","text":"<ul> <li>Guo et al. (2025) \u2014 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. [Link]</li> <li>Analytics Vidhya (2025) \u2014 LLM Optimization: GRPO, PPO, and DPO. [Blog]</li> </ul>"},{"location":"references/#6-reward-hacking-specification-gaming","title":"\ud83e\udde0 6. Reward Hacking &amp; Specification Gaming","text":"<ul> <li>Amodei et al. (2016) \u2014 Concrete Problems in AI Safety. [arXiv:1606.06565]</li> </ul>"},{"location":"references/#others","title":"\ud83e\udde9 Others","text":"<ul> <li>Ouyang et al. (2022) \u2014 Training Language Models to Follow Instructions with Human Feedback (InstructGPT). [arXiv:2203.02155]</li> <li>Bai et al. (2022) \u2014 Training a Helpful and Harmless Assistant with RLHF. [arXiv:2204.05862]</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/","title":"\ud83e\uddee DeepSeek-R1: Reinforcement Learning for Fine-Tuning LLMs","text":"<p>This document provides a detailed overview of the DeepSeek-R1 strategy for fine-tuning and preference-tuning large language models (LLMs). It covers the RL methods, distinctions from traditional approaches, the optimization algorithm (GRPO) summary, multi-stage training pipeline, reward design, model distillation, and additional technical detail.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#1-overview","title":"1. Overview","text":"<p>DeepSeek-R1 introduces a novel approach to improving reasoning capabilities and general instruction-following in large language models using reinforcement learning (RL). Rather than relying solely on large human-annotated supervised fine-tuning datasets and learned reward models, DeepSeek emphasises verifiable tasks (particularly reasoning/maths/code), multi-stage pipelines, and knowledge distillation to smaller models.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#11-key-variants","title":"1.1 Key Variants","text":"<ul> <li>DeepSeek-R1-Zero: RL-only variant without initial supervised fine-tuning (SFT). Uses verifiable reasoning tasks (e.g., math, code, logic) with automatically-computable reward signals.  </li> <li>DeepSeek-R1: Multi-stage pipeline starting with a \u201ccold-start\u201d SFT, followed by reasoning-oriented RL, generation of an SFT dataset from high-quality RL outputs, further SFT fine-tuning, and then a second RL stage for broader instruction-following.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#2-the-grpo-algorithm-overview","title":"2. The GRPO Algorithm: Overview","text":"<p>This section gives a summary of the core optimisation algorithm used in DeepSeek-R1, namely Grouped Relative Policy Optimization (GRPO). For full mathematical details and derivation see the dedicated GRPO Algorithm .</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#21-intuition","title":"2.1 Intuition","text":"<ul> <li>Instead of using a value network (critic) as in PPO, GRPO works by sampling multiple candidate outputs for each prompt and comparing their performance within the group (via ranking).  </li> <li>Encourages responses that outperform peers in the same group, emphasising relative improvement rather than absolute reward magnitudes.  </li> <li>Offers more stable training for LLMs at scale by avoiding the complexity and instability of critic/value training.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#22-key-features","title":"2.2 Key Features","text":"<ul> <li>Group-based candidate sampling (group size \\(G\\), e.g., 8\u201316).  </li> <li>Compute advantage   $$     A_i = \\frac{r_i - \\mathrm{mean}(r_{1..G})}{\\mathrm{std}(r_{1..G})}   $$   where \\(r_i\\) is the reward of candidate \\(o_i\\).  </li> <li>Use PPO-style clipped ratio of new policy vs old policy for each candidate, regularised by a KL-penalty to a reference policy.  </li> <li>Regularisation ensures the policy does not drift too far from the initial/frozen \u201creference\u201d model.  </li> <li>No explicit value (critic) function required \u2014 critical for large-scale LLM fine-tuning.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#3-reward-design","title":"3. Reward Design","text":"<p>DeepSeek splits reward design into two main domains: reasoning-oriented tasks, and general instruction-following tasks.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#31-reasoning-oriented-tasks","title":"3.1 Reasoning-Oriented Tasks","text":"<ul> <li>Correctness: Verified automatically (e.g., solvers verify math answers, compilers/tests verify code solutions).  </li> <li>Chain-of-Thought (CoT) / Format Enforcement: Encourage structured reasoning, for example via tags or designated reasoning segments.  </li> <li>Language Consistency / Style: Penalise language mixing (e.g., mixing English &amp; Mandarin) or incoherent formatting.  </li> <li>Weighted Sum: The overall reward may combine correctness + readability/style metrics.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#32-general-instruction-following-tasks","title":"3.2 General Instruction-Following Tasks","text":"<ul> <li>Use of preference models or mixtures of rule-based checks (for helpfulness/harmlessness/style) and/or learned reward models for broader tasks.  </li> <li>After the reasoning-oriented RL stage, DeepSeek shifts to include these broader tasks so that the model becomes more instruction-capable beyond strictly verifiable reasoning domains.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#4-multi-stage-training-pipeline","title":"4. Multi-Stage Training Pipeline","text":"<p>Here is the step-by-step overview of the training strategy employed by DeepSeek-R1.</p> Stage Description Stage 1: Cold-Start SFT A small curated dataset of chain-of-thought reasoning examples is used to bootstrap the model. Helps stabilise initial policy before heavy RL. Stage 2: Reasoning-Oriented RL Using GRPO on verifiable reasoning tasks (math, code, logic) to drive emergent reasoning capability, e.g., using only RL (DeepSeek-R1-Zero) or RL after SFT (DeepSeek-R1). Stage 3: Rejection Sampling \u2192 SFT Dataset Filter RL-generated outputs by quality/readability; this creates a high-quality dataset for SFT fine-tuning. This helps address issues such as language mixing or readability observed in R1-Zero. Stage 4: Second RL Stage (General Instruction-Following) Expand prompt coverage to include broad instructions and incorporate general reward signals (helpfulness, style, harmlessness). Encourages the model to generalise beyond reasoning tasks. Stage 5: Distillation to Smaller Models Use the high-capability RL-trained model as a teacher, generate reasoning-rich data, then fine-tune smaller student models via SFT on that data (rather than performing full RL on smaller models)."},{"location":"advanced_topics/deepseek_rl_finetuning/#41-pipeline-highlights-additional-details","title":"4.1 Pipeline Highlights / Additional Details","text":"<ul> <li>The reasoning-only RL variant (R1-Zero) demonstrates that emergent reasoning can arise via RL alone (no SFT) but suffers readability/language consistency issues.  </li> <li>For R1 proper, the cold-start SFT acts as a \u201ckick-start\u201d to the policy, improving readability and general language handling before RL.  </li> <li>Distillation step: Available models labelled e.g. DeepSeek-R1-Distill-Qwen-7B, 1.5B, 8B, 14B, 32B, 70B based on Qwen or Llama series.  </li> <li>According to public sources, R1 achieved reasoning performance comparable to OpenAI o1-1217 model on reasoning/multitask benchmarks.  </li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#5-distinctive-features-compared-to-traditional-methods","title":"5. Distinctive Features Compared to Traditional Methods","text":"Feature Conventional RLHF / SFT + RL DeepSeek-R1 Strategy Initial SFT Often uses large human-annotated dataset R1-Zero: none; R1: small cold-start SFT Reward Source Learned reward model (often from human preferences) Reasoning tasks: rule-based correctness + ranking; General tasks: mixture Policy Optimisation PPO (with value network / critic, learned rewards) GRPO (group ranking + clipped ratio + KL penalty) Domain Focus Broad instruction-following from the start Emphasis on reasoning first \u2192 then general instructions Post-RL Dataset Generation Sometimes limited RL outputs \u2192 filtered \u2192 SFT dataset then distillation Distillation to Smaller Models Optional / less emphasised Explicit large \u2192 dataset \u2192 smaller models path Emergence of Reasoning Often via SFT + RL; may require large data/human-labelled Demonstrated via RL alone (R1-Zero), then refined by SFT + RL"},{"location":"advanced_topics/deepseek_rl_finetuning/#6-technical-training-details-additional-depth","title":"6. Technical &amp; Training Details (Additional Depth)","text":"<p>Here are deeper details and notes from publicly available sources.</p>"},{"location":"advanced_topics/deepseek_rl_finetuning/#61-model-sizes-and-releases","title":"6.1 Model Sizes and Releases","text":"<ul> <li>The R1 paper describes two main models: R1-Zero and R1, both built on a 37B-activated parameter MoE architecture with total 671B parameters.  </li> <li>Distilled variants: 1.5B, 7B, 8B, 14B, 32B, 70B parameters based on Qwen2.5 and Llama3 series.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#62-reward-function-sampling-details","title":"6.2 Reward Function &amp; Sampling Details","text":"<ul> <li>For reasoning tasks, the reward function is largely rule-based: checking final answers for correctness, format tags for reasoning sections, penalising language mixing.  </li> <li>Some commentary notes that weaker signals like process rewards (how many reasoning steps) were less effective than outcome rewards (correct answer) in this context.  </li> <li>In sampling / generation for distillation: e.g., for R1-Distill models, generation settings included temperature 0.6, top-p 0.95, 64 responses per query used for pass@1 estimation.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#63-training-and-distillation-strategy","title":"6.3 Training and Distillation Strategy","text":"<ul> <li>The distillation process uses the high-capability teacher model to generate large \u201creasoning-rich\u201d datasets. Student models are then fine-tuned via SFT (not full RL) to inherit reasoning patterns.  </li> <li>Some sources indicate the smaller models may still underperform the teacher, but give much better cost/efficiency trade-offs.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#64-observed-strengths-weaknesses","title":"6.4 Observed Strengths &amp; Weaknesses","text":"<ul> <li>Strength: Emergent reasoning capability via RL, high reasoning benchmark performance.  </li> <li>Weaknesses noted: The R1-Zero model had issues with readability and language mixing (because of skipping SFT).  </li> <li>Distilled models, while efficient, may have some drop in reasoning performance compared to full model.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#7-summary-table","title":"7. Summary Table","text":"Component Role Example / Notes Policy Model (LLM) Learns improved policy via RL DeepSeek-R1, DeepSeek-R1-Zero Reference Model Provides KL regularisation baseline Frozen SFT model (in GRPO) Reward Function Scores responses Correctness, readability, chain-of-thought format Group Size (G) Sampling granularity for GRPO 8\u201316 outputs per prompt (typical) Advantage \\(A_i\\) Relative performance metric within group Normalisation: \\((r_i-\\text{mean})/\\text{std}\\) Objective (GRPO) PPO-style surrogate + KL penalty See GRPO doc for full derivation Training Pipeline Multi-stage (Cold-SFT \u2192 RL \u2192 SFT \u2192 RL \u2192 Distill) Reasoning first, then broad instruction Distillation Transfer reasoning to smaller models Student models 1.5B\u201370B params Goal Efficient reasoning/instruction-fine-tuning Stable RL fine-tuning for large LLMs"},{"location":"advanced_topics/deepseek_rl_finetuning/#8-advantages-limitations","title":"8. Advantages &amp; Limitations","text":""},{"location":"advanced_topics/deepseek_rl_finetuning/#advantages","title":"Advantages","text":"<ul> <li>Emergent reasoning via RL (especially R1-Zero) without relying only on large human-annotated SFT datasets.  </li> <li>Efficient multi-stage training strategy combining SFT, RL, filtering, and distillation.  </li> <li>Verifiable reward signals (correctness + format) reduce noise and training instability.  </li> <li>Distillation path enables smaller, deployable models with reasoning capability \u2014 good for cost/production trade-offs.</li> </ul>"},{"location":"advanced_topics/deepseek_rl_finetuning/#limitations","title":"Limitations","text":"<ul> <li>Transparency of full dataset, hyperparameters, and training cost is limited in public domain.  </li> <li>General instruction-following beyond reasoning may still lag (especially in R1-Zero or smaller distilled variants).  </li> <li>Because smaller models are SFT-only post-distillation, they may not fully retain RL-derived benefits.  </li> <li>Effective reward design and especially filtering of RL outputs remain key; if RL outputs are of low quality, filtering becomes a bottleneck.</li> </ul>"},{"location":"advanced_topics/kl_penalty/","title":"\ud83e\udde9 KL Penalty in Policy Optimization (PO) Algorithms","text":""},{"location":"advanced_topics/kl_penalty/#1-overview","title":"1. Overview","text":"<p>In policy optimization (PO) algorithms such as PPO, DPO, GRPO, and other RL- or preference-based fine-tuning methods, the KL penalty (Kullback\u2013Leibler divergence penalty) acts as a regularization mechanism that prevents the updated policy from drifting too far from the reference policy.</p> <p>This constraint stabilizes training and maintains the linguistic and factual integrity of the base model.</p>"},{"location":"advanced_topics/kl_penalty/#2-what-is-kl-divergence","title":"2. What is KL Divergence?","text":"<p>The Kullback\u2013Leibler divergence measures how one probability distribution diverges from another. For two distributions \\(P\\) and \\(Q\\):</p> \\[ D_{KL}(P \\parallel Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right] \\] <p>In PO contexts:</p> <ul> <li>\\(P = \\pi_\\theta(\\cdot | x)\\): the current fine-tuned policy,</li> <li>\\(Q = \\pi_{\\text{ref}}(\\cdot | x)\\): the reference or base policy.</li> </ul> <p>It quantifies how much the fine-tuned model\u2019s output probabilities deviate from the reference model.</p>"},{"location":"advanced_topics/kl_penalty/#3-kl-penalty-in-the-optimization-objective","title":"3. KL Penalty in the Optimization Objective","text":"<p>To enforce stability, a KL penalty term is added to the training objective:</p> \\[ \\mathcal{L}(\\pi_\\theta) = \\mathbb{E}*{(x, y)} \\left[ r(x, y) - \\beta , D*{KL}(\\pi_\\theta(\\cdot|x) \\parallel \\pi_{\\text{ref}}(\\cdot|x)) \\right] \\] <p>where:</p> <ul> <li>\\(r(x, y)\\): reward or preference-derived score,</li> <li>\\(\\beta\\): KL coefficient controlling penalty strength.</li> </ul> <p>The higher the KL divergence, the stronger the penalty. A well-tuned \\(\\beta\\) balances exploration and stability.</p>"},{"location":"advanced_topics/kl_penalty/#4-computing-the-kl-penalty-in-practice","title":"4. Computing the KL Penalty in Practice","text":"<p>For token-level language models, the KL divergence is computed over the token distributions of the current and reference policies.</p> \\[ D_{KL}(\\pi_\\theta \\parallel \\pi_{\\text{ref}}) = \\sum_t \\pi_\\theta(y_t | x, y_{&lt;t}) \\left[ \\log \\pi_\\theta(y_t | x, y_{&lt;t}) - \\log \\pi_{\\text{ref}}(y_t | x, y_{&lt;t}) \\right] \\] <p>In implementation, we approximate this using sampled sequences:</p> \\[ D_{KL} \\approx \\frac{1}{T} \\sum_{t=1}^{T} \\left( \\log \\pi_\\theta(y_t|x, y_{&lt;t}) - \\log \\pi_{\\text{ref}}(y_t|x, y_{&lt;t}) \\right) \\] <p>This can be computed efficiently by comparing log-probabilities from both models on the same batch of samples.</p>"},{"location":"advanced_topics/kl_penalty/#5-adaptive-kl-control","title":"5. Adaptive KL Control","text":"<p>Instead of fixing \\(\\beta\\), some implementations adapt it dynamically to maintain a target divergence \\(D_{KL}^{\\text{target}}\\).</p> \\[ \\beta \\leftarrow \\beta \\times \\begin{cases} 1.1 &amp; \\text{if } D_{KL} &gt; 1.5 \\times D_{KL}^{\\text{target}} \\\\ 0.9 &amp; \\text{if } D_{KL} &lt; 0.5 \\times D_{KL}^{\\text{target}} \\\\ 1.0 &amp; \\text{otherwise} \\end{cases} \\] <p>This adaptive KL control ensures that:</p> <ul> <li>When the policy diverges too much, the penalty increases.</li> <li>When it remains too conservative, the penalty relaxes.</li> </ul>"},{"location":"advanced_topics/kl_penalty/#6-connection-to-ppo-dpo-and-grpo","title":"6. Connection to PPO, DPO, and GRPO","text":"Algorithm KL Penalty Usage Role PPO Implicitly via clipped objective Controls update step size between policies DPO Explicitly through log-prob differences Aligns with preferences without explicit RL GRPO Similar to DPO but on grouped preference sets Maintains stable relative alignment <p>In all cases, the KL term acts as a trust-region constraint, ensuring that optimization remains close to a known and stable distribution.</p>"},{"location":"advanced_topics/kl_penalty/#7-implementation-example","title":"7. Implementation Example","text":"<pre><code># policy and reference models output log-probabilities\nlogprobs = policy_model.log_prob(actions)\nref_logprobs = ref_model.log_prob(actions)\n\n# compute mean KL divergence\nkl_div = (logprobs - ref_logprobs).mean()\n\n# apply KL penalty\nloss = -(rewards - beta * kl_div)\nloss.backward()\n</code></pre> <p>In language-model fine-tuning, <code>logprobs</code> are computed per token, and the mean or sequence-level KL is used for the penalty term.</p>"},{"location":"advanced_topics/kl_penalty/#8-intuition-and-practical-notes","title":"8. Intuition and Practical Notes","text":"<ul> <li>Why it matters: The KL penalty prevents over-fitting to noisy or narrow reward signals.</li> <li>Relation to trust regions: Functions like a constraint on how far the new policy can move from the old one.</li> <li>Tuning \\(\\beta\\):<ul> <li>Too small \u2192 Model diverges, instability.</li> <li>Too large \u2192 Model under-fits, limited learning.</li> </ul> </li> <li>Monitoring: During training, plotting the KL divergence curve helps ensure stable updates.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/","title":"\ud83e\udde9 Reward Hacking in Policy Optimization (PO) Algorithms","text":""},{"location":"advanced_topics/rewards_hacking/#1-overview","title":"1. Overview","text":"<p>Reward hacking (also called specification gaming) occurs when a policy exploits flaws or blind spots in a reward model to maximize its numeric score, but does not achieve the intended behavior according to human preferences or true objectives.</p> <p>In policy optimization (PO) pipelines like PPO, DPO, or GRPO, reward hacking is a critical issue to monitor and mitigate because it can degrade model quality, safety, and alignment.</p>"},{"location":"advanced_topics/rewards_hacking/#2-why-reward-hacking-happens","title":"2. Why Reward Hacking Happens","text":"<p>Several technical causes contribute to reward hacking:</p> <ul> <li>Proxy mis-specification: The reward model \\(r_\\phi\\) is an imperfect surrogate of the true reward \\(r^\\star\\), causing gradients to favor spurious behaviors.</li> <li>Distributional shift: Policies explore states not covered by the training data, leading the reward model to produce overconfident or inaccurate scores in these regions. Policies then discover high-reward behaviors that are not truly desirable.</li> <li>Optimization artifacts: High learning rates, advantage estimation noise, clipping, or large batch sizes can amplify small errors in the reward model, leading to exaggerated exploitation of minor features.</li> <li>Reward shaping &amp; auxiliary objectives: Handcrafted components can unintentionally incentivize shortcuts.</li> <li>Deterministic exploitation: Policies may collapse into low-entropy modes that reliably exploit loopholes.</li> </ul> <p>Mathematically, the policy maximizes \\(\\mathbb{E}_{\\tau\\sim\\pi_\\theta}[r_\\phi(\\tau)]\\), so any exploitable bias in \\(r_\\phi\\) guides the policy toward unwanted behaviors.</p>"},{"location":"advanced_topics/rewards_hacking/#3-concrete-examples-of-reward-hacking","title":"3. Concrete Examples of Reward Hacking","text":"Example Behavior Reward Effect Notes Token artifact Insert <code>&lt;OK&gt;</code> token frequently Increases reward model score, not human satisfaction Exploits a spurious feature learned by the reward model; does not improve real performance Repetition Repeat phrases or verbose outputs Inflated reward despite reduced readability Often occurs when reward correlates with length or perceived effort rather than content quality Adversarial prompt manipulation Add special tokens to prompts Exploits reward heuristics to gain reward Demonstrates the policy finding loopholes in the input space Overly cautious / safe responses Avoids risky answers entirely May get high safety score but low utility Happens when reward model overweights safety heuristics Misleading formatting or style Adds headers, markdown, or emphasized text unnecessarily Increases reward without improving actual content Exploitation of stylistic correlations in training data Repetition of training data snippets Copies known high-reward text Maximizes reward model score May result in plagiarism-like behavior or hallucinations <p>These examples illustrate how a policy can maximize surrogate reward without improving alignment with true objectives.</p>"},{"location":"advanced_topics/rewards_hacking/#4-consequences-and-symptoms","title":"4. Consequences and Symptoms","text":"<ul> <li>Misaligned high reward: Policies achieve high reward-model scores but perform poorly in human evaluation.</li> <li>Loss of diversity: Mode collapse can reduce the range of behaviors, making outputs repetitive and less useful.</li> <li>Hallucinations or unsafe outputs: Reward-hacked behaviors may increase factual errors or unsafe recommendations.</li> <li>Metric delusion: Metrics used for optimization improve while actual performance on intended tasks stagnates or declines.</li> <li>Reduced trustworthiness: Users and auditors may find outputs unreliable or manipulative.</li> <li>Policy brittleness: Policies may overfit to reward artifacts, leading to unexpected failures in novel scenarios.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#5-detection-metrics","title":"5. Detection Metrics","text":"<p>To identify reward hacking, implement multiple complementary checks:</p> <ul> <li>Reward\u2013Human correlation: Compute Spearman/Pearson correlation between reward and human evaluation scores. Declining correlation may indicate gaming.</li> <li>KL/divergence drift: Monitor \\(D_{KL}(\\pi_\\theta | \\pi_{\\text{ref}})\\) to detect policies diverging excessively from reference behaviors.</li> <li>Reward uncertainty: Track ensemble variance, MC dropout uncertainty, or confidence intervals in reward model predictions.</li> <li>Diversity metrics: Evaluate n-gram diversity, per-token entropy, and sequence-level diversity to detect repetitive behaviors.</li> <li>Adversarial validation: Generate edge-case or adversarial prompts to see if rewards are incorrectly inflated.</li> <li>Top-reward audits: Human review of top-k reward episodes to confirm that high rewards align with desired behaviors.</li> <li>Temporal reward patterns: Check for sudden spikes in reward without corresponding improvements in content quality.</li> <li>Behavioral drift: Compare outputs over time to identify slow drift toward reward-hacked behaviors.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#6-mitigation-strategies","title":"6. Mitigation Strategies","text":""},{"location":"advanced_topics/rewards_hacking/#a-reward-model-improvements","title":"A) Reward Model Improvements","text":"<ul> <li>Adversarial data collection: Label top-reward and high-uncertainty outputs, retrain reward model.</li> <li>Ensembles &amp; uncertainty-aware scoring: Use mean minus standard deviation to penalize high-uncertainty episodes.</li> <li>Calibration &amp; regularization: Temperature scaling, label smoothing.</li> <li>Factorized rewards: Decompose into components (safety, clarity, factuality) to detect exploitation.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#b-policy-side-regularization","title":"B) Policy-Side Regularization","text":"<ul> <li>KL penalty / trust region: Keep policy close to reference; adapt \\(\\beta\\) dynamically.</li> <li>Behavior cloning anchor: Add imitation loss from demonstration dataset.</li> <li>Entropy bonuses: Maintain minimum entropy to prevent deterministic exploitation.</li> <li>Conservative policy optimization: Penalize overestimation of OOD actions.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#c-reward-aware-training","title":"C) Reward-Aware Training","text":"<ul> <li>Penalize high uncertainty in rewards.</li> <li>Early stopping based on human evaluation.</li> <li>Adversarial training to improve robustness.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#d-human-in-the-loop","title":"D) Human-in-the-Loop","text":"<ul> <li>Periodic top-reward audits.</li> <li>Active labeling of high-impact samples.</li> <li>Preference aggregation quality control.</li> </ul>"},{"location":"advanced_topics/rewards_hacking/#7-implementation-notes-best-practices","title":"7. Implementation Notes &amp; Best Practices","text":"<ul> <li>Monitor KL divergence, entropy, and reward-human correlation during training.</li> <li>Retrain reward models periodically with policy-generated data.</li> <li>Tune KL coefficient \\(\\beta\\) conservatively: too small \u2192 divergence; too large \u2192 underfitting.</li> <li>Maintain human-in-the-loop audits for safety and alignment.</li> </ul>"},{"location":"methods/decoupled_rpo/","title":"\ud83e\udde0 Decoupled Reward Policy Optimization (DRPO)","text":""},{"location":"methods/decoupled_rpo/#1-overview","title":"1. Overview","text":"<p>Decoupled Reward Policy Optimization (DRPO) is a reinforcement learning algorithm designed to enhance the reasoning capabilities of Large Language Models (LLMs). It addresses limitations in traditional reinforcement learning methods by decoupling the length-based learning signal of correct rollouts from incorrect ones, preventing penalization of correct but lengthy reasoning processes. This is particularly beneficial for tasks requiring complex reasoning and long-form outputs.</p>"},{"location":"methods/decoupled_rpo/#2-drpo-pipeline","title":"2. DRPO Pipeline","text":"<p>The DRPO framework follows the typical RLHF pipeline with key modifications:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT) </p> <ul> <li>Train a base LLM on high-quality human demonstration data (prompt\u2013response pairs).</li> </ul> </li> <li> <p>Reward Model (RM) Training </p> <ul> <li>Train a model to assign scalar rewards to outputs based on human preferences.</li> </ul> </li> <li> <p>Reinforcement Learning (DRPO) </p> <ul> <li>Fine-tune the policy (SFT model) to maximize predicted rewards from the RM, with decoupled reasoning length consideration.</li> </ul> </li> </ol> <p>\ud83d\udca1 Intuition: DRPO ensures that correct but lengthy responses are appropriately rewarded, allowing the model to generate more complex and accurate outputs.</p>"},{"location":"methods/decoupled_rpo/#3-why-drpo-instead-of-traditional-methods","title":"3. Why DRPO Instead of Traditional Methods?","text":"<ul> <li>Traditional RL methods can penalize correct but long reasoning processes due to group-relative advantage functions.  </li> <li>DRPO decouples the length-based learning signal of correct rollouts from incorrect ones, ensuring valid reasoning is reinforced.  </li> </ul>"},{"location":"methods/decoupled_rpo/#4-drpo-key-concepts","title":"4. DRPO Key Concepts","text":""},{"location":"methods/decoupled_rpo/#41-components","title":"4.1 Components","text":"Component Description Policy Model (\u03c0\u209c) The trainable LLM generating responses. Reward Model (R\u2098) Evaluates outputs, providing scalar rewards. Reference Model (\u03c0\u209c\u2092\u2097\u1d48) Snapshot of policy before update, for stability. Value Function (V\u209c) Estimates expected reward for a given prompt. Advantage (A\u209c) Measures how much better an action is than expected: <code>A\u209c = R\u2098 - V\u209c(s\u209c)</code>"},{"location":"methods/decoupled_rpo/#42-intuition","title":"4.2 Intuition","text":"<ul> <li>Generates outputs \u2192 reward model evaluates \u2192 advantage guides update.  </li> <li>Decoupled objective ensures correct but lengthy reasoning is rewarded.</li> </ul>"},{"location":"methods/decoupled_rpo/#5-drpo-objective-function","title":"5. DRPO Objective Function","text":"<p>\ud83d\udcd8 DRPO Mathematical Formulation</p>"},{"location":"methods/decoupled_rpo/#51-probability-ratio","title":"5.1 Probability Ratio","text":"<p>Measures how much the new policy\u2019s likelihood of an action changes compared to the old policy:</p> \\[ r\u209c(\u03c0\u209c) = \\frac{\u03c0\u209c(a\u209c | s\u209c)}{\u03c0\u209c\u2092\u2097\u1d48(a\u209c | s\u209c)} \\]"},{"location":"methods/decoupled_rpo/#52-decoupled-surrogate-loss","title":"5.2 Decoupled Surrogate Loss","text":"<p>Ensures stable updates while rewarding correct reasoning lengths:</p> \\[ L^{DRPO}(\u03c0\u209c) = \\mathbb{E}[ \\min(r\u209c(\u03c0\u209c) A\u209c, \\text{clip}(r\u209c(\u03c0\u209c), 1 - \u03b5, 1 + \u03b5) A\u209c)] \\] <p>Where:</p> <ul> <li>\\(A_t\\): Advantage function \u2014 how much better an action is than expected.  </li> <li>\\(\\epsilon\\): Clipping threshold (typically 0.1\u20130.3).  </li> </ul>"},{"location":"methods/decoupled_rpo/#6-value-function-advantage-and-reward-computation","title":"6. Value Function, Advantage, and Reward Computation","text":"<p>\ud83d\udcd7 Supporting Mathematical Components</p>"},{"location":"methods/decoupled_rpo/#61-cumulative-reward-return","title":"6.1 Cumulative Reward (Return)","text":"\\[ R\u209c = \\sum_{k=0}^{\u221e} \u03b3^k r\u209c\u208a\u2096 \\] <ul> <li>\\(r\u209c\\): reward received at time \\(t\\) (from reward model).  </li> <li>\\(\u03b3\\): discount factor (typically 0.95\u20130.99).  </li> </ul> <p>In RLHF, responses are often short, so sequence-level rewards are used.</p>"},{"location":"methods/decoupled_rpo/#62-value-function","title":"6.2 Value Function","text":"\\[ V\u209c(s\u209c) \u2248 \\mathbb{E}[R\u209c | s\u209c] \\] <p>Value loss:</p> \\[ L^{value}(\u03c0\u209c) = \\frac{1}{2} (V\u209c(s\u209c) - R\u209c)^2 \\]"},{"location":"methods/decoupled_rpo/#63-advantage-function","title":"6.3 Advantage Function","text":"\\[ A\u209c = R\u209c - V\u209c(s\u209c) \\] <p>Optionally, Generalized Advantage Estimation (GAE):</p> \\[ A\u209c = \\sum_{l=0}^{\u221e} (\u03b3\u03bb)^l \u03b4\u209c\u208a\u2097 \\] <p>Where:</p> <ul> <li>\\(\u03b4\u209c = r\u209c + \u03b3 V\u209c(s\u209c\u208a\u2081) \u2212 V\u209c(s\u209c)\\) </li> <li>\\(\u03bb\\) = GAE smoothing factor (0.9\u20130.97)</li> </ul>"},{"location":"methods/decoupled_rpo/#64-entropy-bonus","title":"6.4 Entropy Bonus","text":"\\[ L^{entropy}(\u03c0\u209c) = - \\sum_a \u03c0\u209c(a|s\u209c) \\log \u03c0\u209c(a|s\u209c) \\] <p>Promotes exploration and diversity in responses.</p>"},{"location":"methods/decoupled_rpo/#65-combined-drpo-loss","title":"6.5 Combined DRPO Loss","text":"\\[ L_{total}(\u03c0\u209c) = -L^{DRPO}(\u03c0\u209c) + c\u2081 \\cdot L^{value}(\u03c0\u209c) - c\u2082 \\cdot H[\u03c0\u209c] \\] <p>Where \\(H[\u03c0\u209c]\\) is the entropy term; \\(c\u2081, c\u2082\\) control loss weighting.</p>"},{"location":"methods/decoupled_rpo/#7-iterative-drpo-update","title":"7. Iterative DRPO Update","text":"<ol> <li>Generate response with policy model.  </li> <li>Compute reward using reward model.  </li> <li>Compute log probabilities (old vs new policy).  </li> <li>Estimate value using value head.  </li> <li>Compute advantage.  </li> <li>Update policy using clipped surrogate loss.  </li> <li>Update value function.  </li> <li>Apply entropy bonus.  </li> <li>Update reference model.</li> </ol> <p>\u2705 DRPO updates only when new behavior is better and within a controlled region, ensuring valid reasoning lengths are rewarded.</p>"},{"location":"methods/decoupled_rpo/#8-implementation-example-pseudocode","title":"8. Implementation Example (Pseudocode)","text":"<pre><code>for prompt in prompts:\n    response = policy_model.generate(prompt)\n    reward = reward_model(prompt, response)\n\n    logprobs_old = ref_model.logprobs(prompt, response)\n    logprobs_new = policy_model.logprobs(prompt, response)\n\n    value = value_head(prompt)\n\n    advantage = reward - value  # sequence-level\n\n    ratio = torch.exp(logprobs_new - logprobs_old)\n\n    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n\n    policy_loss = -torch.mean(torch.min(ratio * advantage, clipped_ratio * advantage))\n\n    value_loss = 0.5 * (value - reward) ** 2\n\n    entropy = -torch.sum(torch.exp(logprobs_new) * logprobs_new)\n    entropy_coeff = 0.01\n\n    total_loss = policy_loss + 0.5 * value_loss - entropy_coeff * entropy\n\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    ref_model.load_state_dict(policy_model.state_dict())\n</code></pre>"},{"location":"methods/decoupled_rpo/#9-limitations-and-challenges","title":"9. Limitations and Challenges","text":"<ol> <li>KL Divergence Sensitivity: Requires KL penalty to prevent divergence.  </li> <li>High Training Cost: Multiple models increase compute.  </li> <li>Reward Hacking: LLM may over-optimize for reward model.  </li> <li>Sparse/Noisy Rewards: One reward per sequence can hinder credit assignment.  </li> <li>Credit Assignment: Per-token updates with sequence-level rewards may be ambiguous.  </li> <li>Exploration vs Alignment: Balancing safe exploration is challenging.  </li> <li>Implementation Complexity: Multiple models and hyperparameter tuning required.</li> </ol>"},{"location":"methods/decoupled_rpo/#10-summary","title":"10. Summary","text":"Component Role Example Policy Model (LLM) Generates responses GPT-3, LLaMA-2 Reward Model Scores outputs based on preferences Fine-tuned classifier DRPO Algorithm Updates policy with decoupled reward Training loop KL Penalty Prevents over-deviation Regularization Goal Align LLM behavior with human intent Helpful, safe, accurate reasoning"},{"location":"methods/dpo/","title":"\ud83e\udde9 Direct Preference Optimization (DPO) \u2014 Reinforcement Learning-Free Alignment","text":""},{"location":"methods/dpo/#1-overview","title":"1. Overview","text":"<p>Direct Preference Optimization (DPO) is an algorithm designed to fine-tune Large Language Models (LLMs) using human preference data \u2014 without requiring a separate reward model or reinforcement learning (RL) loop. It directly learns from pairs of preferred and rejected responses, offering a simpler and more stable alternative to Proximal Policy Optimization (PPO) in the Reinforcement Learning from Human Feedback (RLHF) pipeline.</p>"},{"location":"methods/dpo/#2-the-big-picture-from-rlhf-to-dpo","title":"2. The Big Picture: From RLHF to DPO","text":"<p>While traditional RLHF involves three stages \u2014 Supervised Fine-Tuning (SFT), Reward Model (RM) Training, and PPO Fine-Tuning \u2014 DPO collapses the latter two into a single, direct optimization step.</p> Stage PPO-Based RLHF DPO-Based Alignment 1\ufe0f\u20e3 SFT Train base LLM on human demonstrations \u2705 Same 2\ufe0f\u20e3 RM Train reward model on preference pairs \u274c Not needed 3\ufe0f\u20e3 RL Fine-tune using PPO + rewards \u2705 Replaced by DPO objective <p>This makes DPO computationally lighter, easier to implement, and more stable.</p>"},{"location":"methods/dpo/#3-intuitive-understanding","title":"3. Intuitive Understanding","text":"<p>Imagine training an assistant:</p> <ul> <li>PPO: The assistant writes an answer \u2192 a teacher scores it numerically (via a reward model) \u2192 updates happen using RL.</li> <li>DPO: The assistant sees two answers for the same question \u2014 one good, one bad \u2014 and learns which is better.</li> </ul> <p>Thus, DPO bypasses numeric rewards and learns preferences directly.</p>"},{"location":"methods/dpo/#4-training-data-and-setup","title":"4. Training Data and Setup","text":"<p>Each DPO training example consists of \\((x, y^+, y^-)\\)</p> <p>where:</p> <ul> <li>\\(x\\): Prompt or input query</li> <li>\\(y^+\\): Preferred (chosen) response</li> <li>\\(y^-\\): Less preferred (rejected) response</li> </ul> <p>The model learns to assign higher probability to \\(y^+\\) than \\(y^-\\), while staying close to a reference model \\(\\pi_{\\text{ref}}\\) (usually the SFT model).</p>"},{"location":"methods/dpo/#5-dpo-formulation","title":"5. DPO Formulation","text":"<p>\ud83d\udcd8 Mathematical Formulation</p>"},{"location":"methods/dpo/#51-objective-function","title":"5.1. Objective Function","text":"<p>DPO reframes preference optimization as a direct likelihood-ratio objective, eliminating the need for an explicit reward model or reinforcement learning loop. The resulting closed-form objective is:</p> \\[ L_{\\mathrm{DPO}}(\\theta) = -\\mathbb{E}_{(x, y^+, y^-)} \\left[ \\log \\sigma \\left( \\beta \\Big[ (\\log \\pi_\\theta(y^+|x) - \\log \\pi_{\\text{ref}}(y^+|x)) - (\\log \\pi_\\theta(y^-|x) - \\log \\pi_{\\text{ref}}(y^-|x)) \\Big] \\right) \\right] \\] <p>where:</p> <ul> <li>\\(\\pi_\\theta\\): Trainable policy model</li> <li>\\(\\pi_{\\text{ref}}\\): Frozen reference model (often the SFT model)</li> <li>\\(\\sigma\\): Sigmoid function</li> <li>\\(\\beta\\): Inverse temperature hyperparameter controlling the tradeoff between alignment strength and faithfulness to the reference model</li> </ul>"},{"location":"methods/dpo/#52-intuition","title":"5.2. Intuition","text":"<p>The objective encourages the model to increase the likelihood of preferred responses \\(y^+\\) relative to dispreferred ones \\(y^-\\), while regularizing against divergence from the reference policy.</p> <p>This can be interpreted as implicitly performing reward-based optimization, with the implicit reward function defined as:</p> \\[ r_\\theta(x, y) = \\beta \\big[ \\log \\pi_\\theta(y|x) - \\log \\pi_{\\text{ref}}(y|x) \\big] \\] <p>This formulation shows that DPO optimizes the same relative preferences that PPO would learn from a reward model \u2014 but in a single forward pass, without explicit reward modeling or KL penalty terms. Hence the popular phrase:</p> <p>\u201cYour language model is secretly a reward model.\u201d</p>"},{"location":"methods/dpo/#53-implementation-details-and-best-practices","title":"5.3. Implementation Details and Best Practices","text":"<ul> <li>Reference model is frozen \u2014 do not allow gradient flow into \\(\\pi_{\\text{ref}}\\).</li> <li>Sequence-level log-probabilities \u2014 compute \\(\\log \\pi(y|x)\\) as the sum (or mean) of token log-probabilities for the entire response.</li> <li>Length normalization \u2014 optional, but useful if \\(y^+\\) and \\(y^-\\) differ in length.</li> <li>Numerical stability \u2014 use stable forms such as <code>-F.logsigmoid(logits)</code> in PyTorch rather than raw <code>log(sigmoid(...))</code>.</li> <li>\u03b2 (beta) \u2014 higher \u03b2 increases divergence from the reference; small \u03b2 keeps the model closer to the base. Typical values: 0.1\u20130.5.</li> <li>Training step \u2014</li> </ul> <pre><code>logits = beta * ((logp_pos - logp_pos_ref) - (logp_neg - logp_neg_ref))\nloss = -torch.logsigmoid(logits).mean()\n</code></pre> <ul> <li>Consistent tokenization \u2014 ensure both \\(\\pi_\\theta\\) and \\(\\pi_{\\text{ref}}\\) use the same tokenizer and decoding setup.</li> <li>Regularization monitoring \u2014 even though DPO has implicit KL regularization, tracking the KL divergence between \\(\\pi_\\theta\\) and \\(\\pi_{\\text{ref}}\\) helps prevent over-drift.</li> </ul>"},{"location":"methods/dpo/#54-key-takeaways","title":"5.4. Key Takeaways","text":"<ul> <li>DPO avoids explicit reward models and RL optimization loops.</li> <li>It implicitly aligns model preferences through likelihood ratios.</li> <li>The \u03b2 parameter provides a smooth knob between faithfulness and alignment strength.</li> <li>Simpler, more stable, and often more data-efficient than PPO while achieving comparable alignment.</li> </ul>"},{"location":"methods/dpo/#6-implementation-example-pseudocode","title":"6. Implementation Example (Pseudocode)","text":"<pre><code>for (prompt, pos, neg) in preference_dataset:\n    # Compute log-probabilities for chosen and rejected responses\n    logp_pos = model.logprobs(prompt, pos)\n    logp_neg = model.logprobs(prompt, neg)\n\n    # Reference model log-probabilities\n    logp_pos_ref = ref_model.logprobs(prompt, pos)\n    logp_neg_ref = ref_model.logprobs(prompt, neg)\n\n    # Compute logits and DPO loss\n    logits = beta * ((logp_pos - logp_neg) - (logp_pos_ref - logp_neg_ref))\n    loss = -torch.log(torch.sigmoid(logits)).mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"methods/dpo/#7-why-dpo-instead-of-ppo","title":"7. Why DPO Instead of PPO?","text":"Aspect PPO DPO Reward Model Requires separate RM Not needed RL Loop Yes (policy + value optimization) No KL Penalty Manually tuned Implicitly handled via reference model Training Stability Sensitive to hyperparameters More stable Complexity High (multiple models: policy, RM, value) Simple (policy + reference only) Data Efficiency Uses scalar rewards Uses preference pairs directly Computation Cost Expensive Lightweight"},{"location":"methods/dpo/#8-limitations-and-challenges","title":"8. Limitations and Challenges","text":""},{"location":"methods/dpo/#1-limited-preference-data","title":"\ud83d\udcc9 1. Limited Preference Data","text":"<p>High-quality pairwise preference datasets are expensive to collect at scale.</p>"},{"location":"methods/dpo/#2-generalization-gaps","title":"\ud83d\udd04 2. Generalization Gaps","text":"<p>DPO may overfit to the preference distribution and underperform on unseen prompt styles.</p>"},{"location":"methods/dpo/#3-reference-model-sensitivity","title":"\u2696\ufe0f 3. Reference Model Sensitivity","text":"<p>If the reference model is too weak or too strong, DPO optimization can become unstable.</p>"},{"location":"methods/dpo/#4-no-explicit-reward-signal","title":"\ud83e\udde9 4. No Explicit Reward Signal","text":"<p>Without continuous reward signals, DPO can struggle to explore novel or creative answers.</p>"},{"location":"methods/dpo/#5-human-noise-amplification","title":"\ud83c\udfad 5. Human Noise Amplification","text":"<p>Inconsistent or biased human feedback can directly affect model preference alignment.</p>"},{"location":"methods/dpo/#9-summary-table","title":"9. Summary Table","text":"Component Role Example Policy Model (LLM) Learns preferences directly <code>GPT-3</code>, <code>Llama-2</code> Reference Model Provides baseline probabilities SFT model DPO Objective Increases likelihood of preferred responses Log-sigmoid loss \u03b2 Parameter Controls proximity to reference Tuning hyperparameter Goal Align behavior with human preferences Stable, lightweight alignment"},{"location":"methods/drpo/","title":"\ud83c\udfaf Direct Reward Policy Optimization (DRPO) \u2014 Offline Reward-Weighted Alignment","text":""},{"location":"methods/drpo/#1-overview","title":"1. Overview","text":"<p>Direct Reward Policy Optimization (DRPO) is an algorithm designed to fine-tune Large Language Models (LLMs) using reward signals\u2014without requiring an online reinforcement learning (RL) loop. It directly leverages reward model scores as weights in a supervised training objective, offering a simpler and more stable alternative to Proximal Policy Optimization (PPO) while being more expressive than Direct Preference Optimization (DPO).</p>"},{"location":"methods/drpo/#2-the-big-picture-from-ppo-to-drpo","title":"2. The Big Picture: From PPO to DRPO","text":"<p>While PPO-based RLHF uses iterative, online reinforcement learning guided by a reward model, DRPO turns this process into a single offline optimization step.</p> Stage PPO-Based RLHF DRPO-Based Alignment 1\ufe0f\u20e3 SFT Train base LLM on human demonstrations \u2705 Same 2\ufe0f\u20e3 RM Train reward model on preference data \u2705 Same 3\ufe0f\u20e3 RL Fine-tune using PPO + rewards \u2705 Replaced by DRPO objective <p>This makes DRPO computationally cheaper, offline, and more stable, while retaining reward sensitivity.</p>"},{"location":"methods/drpo/#3-intuitive-understanding","title":"3. Intuitive Understanding","text":"<p>Imagine training an assistant:</p> <ul> <li>PPO: The assistant generates an answer \u2192 a teacher scores it numerically \u2192 the model updates using RL.</li> <li>DRPO: The assistant looks at many answers already scored \u2192 learns to favor high-reward answers through weighted likelihood training.</li> </ul> <p>Thus, DRPO bypasses the complex RL loop but still uses explicit reward signals for alignment.</p>"},{"location":"methods/drpo/#4-training-data-and-setup","title":"4. Training Data and Setup","text":"<p>Each DRPO training example consists of \\((x, y, r)\\)</p> <p>where:</p> <ul> <li>\\(x\\): Prompt or input query</li> <li>\\(y\\): Candidate response (generated by the base model)</li> <li>\\(r\\): Scalar reward value from the reward model</li> </ul> <p>The model is fine-tuned to increase the likelihood of high-reward responses while staying close to a reference model \\(\\pi_{\\text{ref}}\\) (often the SFT model).</p>"},{"location":"methods/drpo/#5-drpo-formulation","title":"5. DRPO Formulation","text":"<p>\ud83d\udcd8 Mathematical Formulation</p>"},{"location":"methods/drpo/#51-objective-function","title":"5.1. Objective Function","text":"<p>DRPO formulates offline alignment as a reward-weighted log-likelihood objective with KL regularization:</p> \\[ L_{\\mathrm{DRPO}}(\\theta) = -\\mathbb{E}_{(x, y)} \\left[ r(x, y) \\, \\log \\pi_\\theta(y|x) \\right] + \\beta \\, D_{\\mathrm{KL}}\\!\\left( \\pi_\\theta(\\cdot|x) \\, \\| \\, \\pi_{\\text{ref}}(\\cdot|x) \\right) \\] <p>where:</p> <ul> <li>\\(\\pi_\\theta\\): Trainable policy model  </li> <li>\\(\\pi_{\\text{ref}}\\): Reference (frozen) model  </li> <li>\\(r(x, y)\\): Reward score from the reward model  </li> <li>\\(\\beta\\): KL regularization coefficient controlling drift from the reference model</li> </ul>"},{"location":"methods/drpo/#52-intuition","title":"5.2. Intuition","text":"<p>DRPO can be viewed as reward-weighted maximum likelihood estimation (MLE) \u2014 it learns a distribution that assigns higher probability mass to high-reward samples.</p> <p>The KL term maintains stability and prevents the model from overfitting or diverging too far from the reference model.</p> <p>Implicitly, this mirrors the PPO objective: $$ L_{\\mathrm{PPO}} \\approx \\mathbb{E}!\\left[ A_t \\frac{\\pi_\\theta(y_t|x_t)}{\\pi_{\\text{old}}(y_t|x_t)} - \\beta \\, D_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\text{old}}) \\right] $$ but replaces advantage estimates \\(A_t\\) with offline rewards \\(r(x, y)\\) and removes the online loop.</p>"},{"location":"methods/drpo/#53-practical-implementation-notes-gotchas","title":"5.3. Practical Implementation Notes / Gotchas","text":"<ul> <li>Reference model is frozen \u2014 no gradient flow into \\(\\pi_{\\text{ref}}\\).  </li> <li>Reward normalization \u2014 normalize \\(r(x, y)\\) across batches to prevent gradient explosion.  </li> <li>Sequence log-probabilities \u2014 compute \\(\\log \\pi(y|x)\\) at sequence level (sum or mean over tokens).  </li> <li>KL monitoring \u2014 track drift between \\(\\pi_\\theta\\) and \\(\\pi_{\\text{ref}}\\) during training.  </li> <li>\u03b2 (beta) \u2014 higher \u03b2 keeps the model closer to the base; typical range: 0.1\u20130.5.  </li> <li>Numerical stability \u2014 use stable PyTorch ops like <code>F.kl_div</code> or <code>F.log_softmax</code> to avoid underflow.  </li> <li>Dataset quality \u2014 ensure sufficient diversity in reward-labeled responses to avoid overfitting.</li> </ul>"},{"location":"methods/drpo/#54-key-takeaways","title":"5.4. Key Takeaways","text":"<ul> <li>DRPO avoids online rollouts \u2014 all data is precomputed.  </li> <li>Retains reward awareness (unlike DPO) while remaining stable (unlike PPO).  </li> <li>Functions as a bridge between supervised fine-tuning and reinforcement learning.  </li> <li>Efficient for large-scale alignment when PPO is too costly.</li> </ul>"},{"location":"methods/drpo/#6-implementation-example-pseudocode","title":"6. Implementation Example (Pseudocode)","text":"<pre><code>for (prompt, response, reward) in reward_dataset:\n    logp = model.logprobs(prompt, response)\n    logp_ref = ref_model.logprobs(prompt, response)\n\n    kl = (logp - logp_ref).mean()\n    loss = -(reward * logp).mean() + beta * kl\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"methods/drpo/#7-drpo-vs-other-methods","title":"7. DRPO vs. Other Methods","text":""},{"location":"methods/drpo/#71-drpo-vs-ppo","title":"7.1. DRPO vs. PPO","text":"Aspect PPO DRPO Training loop Online, iterative Offline, single-pass Reward usage Advantage-based, dynamic Reward-weighted, static Exploration Strong (on-policy) Limited (SFT data only) Stability Unstable without tuning Very stable Cost High 5\u201320\u00d7 cheaper Performance Slightly higher ~90\u201395% of PPO"},{"location":"methods/drpo/#72-drpo-vs-dpo","title":"7.2. DRPO vs. DPO","text":"Aspect DPO DRPO Input Preference pairs Reward scores Feedback Binary preferences Scalar rewards Loss Logistic (pairwise) Weighted log-likelihood Reward model Not needed Required Exploration None Some (via reward diversity) Behavior Learns which is better Learns how much better"},{"location":"methods/drpo/#8-limitations-and-challenges","title":"8. Limitations and Challenges","text":""},{"location":"methods/drpo/#1-reward-model-bias","title":"\ud83d\udcc9 1. Reward Model Bias","text":"<p>DRPO inherits any biases or inaccuracies from the reward model, which may skew optimization.</p>"},{"location":"methods/drpo/#2-limited-exploration","title":"\ud83d\udd04 2. Limited Exploration","text":"<p>Since DRPO is offline, it can only improve on responses generated by the base (SFT) model.</p>"},{"location":"methods/drpo/#3-kl-tuning-sensitivity","title":"\u2696\ufe0f 3. KL Tuning Sensitivity","text":"<p>Improper \u03b2 settings can lead to over-fitting (low \u03b2) or under-fitting (high \u03b2).</p>"},{"location":"methods/drpo/#4-reward-over-fitting","title":"\ud83e\udde0 4. Reward Over-fitting","text":"<p>Over-optimizing for the reward model can cause reward hacking \u2014 responses that exploit model scoring patterns.</p>"},{"location":"methods/drpo/#5-data-requirement","title":"\ud83d\udcbe 5. Data Requirement","text":"<p>Requires a sufficiently large and diverse dataset of reward-labeled responses for robust alignment.</p>"},{"location":"methods/drpo/#9-summary-table","title":"9. Summary Table","text":"Component Role Example Policy Model (LLM) Learns weighted preferences <code>GPT-3</code>, <code>Llama-2</code> Reference Model Provides baseline probabilities SFT model Reward Model Provides scalar scores for responses RM trained from human prefs DRPO Objective Reward-weighted log-likelihood + KL penalty Offline alignment objective \u03b2 Parameter Controls proximity to reference Tuning hyperparameter Goal Efficient reward-based alignment Stable, cost-effective tuning"},{"location":"methods/drpo/#10-when-to-use-drpo","title":"10. When to Use DRPO","text":"<ul> <li>You already have a trained reward model.</li> <li>You need offline, stable, and cost-efficient alignment.</li> <li>You want to bridge between SFT and PPO without running expensive RL loops.</li> <li>Ideal for intermediate or large-scale pre-alignment before PPO fine-tuning.</li> </ul>"},{"location":"methods/grouped_relative_po_deepseek/","title":"\ud83e\uddee Grouped Relative Policy Optimization (GRPO) \u2014 Reinforcement Learning for Efficient LLM Alignment","text":""},{"location":"methods/grouped_relative_po_deepseek/#1-overview","title":"1. Overview","text":"<p>Grouped Relative Policy Optimization (GRPO) is a reinforcement learning algorithm introduced in the DeepSeek series (DeepSeekMath, DeepSeek-R1) to fine-tune Large Language Models (LLMs) efficiently on reasoning-intensive tasks. Unlike traditional PPO, which requires a critic (value network), GRPO eliminates the critic and computes relative advantages within groups of sampled outputs. This approach reduces computational cost and stabilizes training, making it well-suited for large-scale language model alignment.</p>"},{"location":"methods/grouped_relative_po_deepseek/#2-the-big-picture-from-ppo-to-grpo","title":"2. The Big Picture: From PPO to GRPO","text":"<p>Traditional RLHF pipelines (using PPO) require a policy model, a reward model, and a value function. GRPO simplifies this process by using group-wise relative advantages instead of an explicit value estimator.</p> Stage PPO-Based RLHF GRPO-Based Alignment 1\ufe0f\u20e3 SFT Train base LLM on human demonstrations \u2705 Same 2\ufe0f\u20e3 RM Train reward or value model \u274c Removed (uses reward function directly) 3\ufe0f\u20e3 RL Fine-tune using PPO updates \u2705 Fine-tune using group-based GRPO objective <p>This design significantly reduces training instability and memory usage while preserving the benefits of policy-gradient fine-tuning.</p>"},{"location":"methods/grouped_relative_po_deepseek/#3-intuitive-understanding","title":"3. Intuitive Understanding","text":"<p>For each prompt, GRPO samples G candidate responses from the old policy, evaluates each response using a reward function, and compares them within the group. The model then updates its policy to favor responses that outperform others in the same group \u2014 a relative rather than absolute improvement process.</p> <p>Intuitively:</p> <ul> <li>PPO optimizes each response using absolute advantages from a critic.</li> <li>GRPO optimizes by ranking multiple sampled responses and pushing the policy toward higher-ranked ones.</li> </ul> <p>This allows GRPO to focus on comparative improvement while maintaining diversity and avoiding overfitting to noisy rewards.</p>"},{"location":"methods/grouped_relative_po_deepseek/#4-training-data-and-setup","title":"4. Training Data and Setup","text":"<p>Each GRPO training example includes:</p> <ul> <li>Prompt: \\( q \\)</li> <li>Group of outputs: \\( \\{o_1, o_2, \\dots, o_G\\} \\) sampled from the old policy \\( \\pi_{\\text{old}} \\)</li> <li>Reward values: \\( r_i = r(q, o_i) \\) from a scoring or reward function</li> </ul> <p>The policy model \\( \\pi_\\theta \\) is optimized to assign higher probabilities to outputs with higher relative rewards, regularized by a KL penalty with respect to a frozen reference policy \\( \\pi_{\\text{ref}} \\).</p>"},{"location":"methods/grouped_relative_po_deepseek/#5-grpo-formulation","title":"5. GRPO Formulation","text":"<p>\ud83d\udcd8 Mathematical Formulation</p>"},{"location":"methods/grouped_relative_po_deepseek/#51-objective-function","title":"5.1. Objective Function","text":"<p>GRPO generalizes the PPO objective using group-wise normalization:</p> \\[ J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{q, \\{o_i\\}} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\min \\Big(   \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\text{old}}(o_i|q)} A_i,\\,   \\text{clip}\\!\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\text{old}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon \\right) A_i \\Big) - \\beta\\, D_{\\mathrm{KL}}\\!\\big(\\pi_\\theta \\| \\pi_{\\text{ref}}\\big) \\right] \\] <p>where:</p> <ul> <li>\\( \\pi_{\\text{old}} \\): policy before update  </li> <li>\\( A_i \\): normalized advantage within the group  </li> <li>\\( \\epsilon \\): PPO clipping coefficient  </li> <li>\\( \\beta \\): KL regularization coefficient  </li> <li>\\( \\pi_{\\text{ref}} \\): frozen reference model</li> </ul>"},{"location":"methods/grouped_relative_po_deepseek/#52-grouped-advantage","title":"5.2. Grouped Advantage","text":"<p>The relative advantage \\(A_i\\) is computed within each group:</p> \\[ A_i = \\frac{r_i - \\mathrm{mean}(r_{1..G})}{\\mathrm{std}(r_{1..G})} \\] <p>where \\(r_i\\) is the reward for output \\(o_i\\). This ensures that updates depend on relative performance rather than absolute reward magnitude.</p>"},{"location":"methods/grouped_relative_po_deepseek/#53-kl-regularization","title":"5.3. KL Regularization","text":"<p>The KL term ensures that the updated policy remains close to the reference model:</p> \\[ D_{\\mathrm{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - \\log \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1 \\]"},{"location":"methods/grouped_relative_po_deepseek/#54-intuition","title":"5.4. Intuition","text":"<ul> <li>Group-normalized advantages remove the need for a critic.  </li> <li>KL regularization replaces the explicit PPO penalty term.  </li> <li>Clipping prevents large, unstable policy updates.  </li> <li>Efficiency: GRPO avoids computing value baselines, making it highly scalable for LLMs.</li> </ul>"},{"location":"methods/grouped_relative_po_deepseek/#55-implementation-details","title":"5.5. Implementation Details","text":"<ul> <li>Group size (G) \u2014 Typically 8\u201316 samples per prompt.  </li> <li>\u03b2 (beta) \u2014 0.001\u20130.01 to control KL regularization.  </li> <li>\u03b5 (epsilon) \u2014 Clipping coefficient, often 0.1\u20130.2.  </li> <li>Reference policy \u2014 Frozen SFT model to anchor learning.  </li> <li>Reward function \u2014 Task-specific (e.g., correctness, coherence, reasoning completeness).  </li> <li>Advantage normalization \u2014 Essential for stable updates; normalize per group.</li> </ul>"},{"location":"methods/grouped_relative_po_deepseek/#6-implementation-example-pseudocode","title":"6. Implementation Example (Pseudocode)","text":"<pre><code>for prompt in dataset:\n    outputs = [policy_old.generate(prompt) for _ in range(G)]\n    rewards = [reward_fn(prompt, o) for o in outputs]\n    mean_r, std_r = np.mean(rewards), np.std(rewards) + 1e-8\n    advantages = [(r - mean_r) / std_r for r in rewards]\n\n    logp_old = [policy_old.logprob(prompt, o) for o in outputs]\n    logp_new = [policy.logprob(prompt, o) for o in outputs]\n\n    ratios = [torch.exp(lp_new - lp_old) for lp_new, lp_old in zip(logp_new, logp_old)]\n    surr = [torch.min(r * A, torch.clamp(r, 1-\u03b5, 1+\u03b5) * A)\n            for r, A in zip(ratios, advantages)]\n\n    loss_policy = -torch.mean(torch.stack(surr))\n    kl_loss = \u03b2 * compute_KL(policy, ref_policy, prompt, outputs)\n    loss = loss_policy + kl_loss\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"methods/grouped_relative_po_deepseek/#7-why-grpo-instead-of-ppo","title":"7. Why GRPO Instead of PPO?","text":"Aspect PPO GRPO Critic / Value Net Required \u274c Removed Advantage Computation From value estimates (GAE) Group-normalized rewards KL Regularization Explicit or adaptive penalty Included via reference policy Training Stability Sensitive to critic/value bias More stable and memory-efficient Data Efficiency Uses single rollout per update Leverages multiple outputs per prompt Compute Cost High (policy + value models) Low (policy-only) Suitability General RL tasks LLM fine-tuning with verifiable rewards"},{"location":"methods/grouped_relative_po_deepseek/#8-limitations-and-challenges","title":"8. Limitations and Challenges","text":""},{"location":"methods/grouped_relative_po_deepseek/#1-group-reward-homogeneity","title":"\ud83d\udcc9 1. Group Reward Homogeneity","text":"<p>If all responses in a group have similar rewards, normalized advantages vanish, yielding weak gradients.</p>"},{"location":"methods/grouped_relative_po_deepseek/#2-reward-function-quality","title":"\ud83d\udd04 2. Reward Function Quality","text":"<p>GRPO still relies on reward signal design; noisy or biased rewards can misguide optimization.</p>"},{"location":"methods/grouped_relative_po_deepseek/#3-kl-coefficient-sensitivity","title":"\u2696\ufe0f 3. KL Coefficient Sensitivity","text":"<p>If \u03b2 is too small, the model may drift from the base; too large, and updates stall.</p>"},{"location":"methods/grouped_relative_po_deepseek/#4-group-size-tradeoff","title":"\ud83d\udca1 4. Group Size Tradeoff","text":"<p>Larger groups improve ranking precision but increase compute cost.</p>"},{"location":"methods/grouped_relative_po_deepseek/#5-limited-exploration","title":"\ud83c\udfad 5. Limited Exploration","text":"<p>As with PPO, GRPO may struggle to explore novel or diverse outputs if rewards are narrow.</p>"},{"location":"methods/grouped_relative_po_deepseek/#9-summary-table","title":"9. Summary Table","text":"Component Role Example Policy Model (LLM) Learns improved policy via group comparison DeepSeek-R1, DeepSeekMath Reference Model Provides KL regularization baseline Frozen SFT model Reward Function Scores responses Correctness, Coherence, Style, etc. Group Size (G) Defines sampling granularity 8\u201316 outputs Advantage \\(A_i\\) Relative performance metric Normalized per group Objective PPO-like surrogate + KL penalty Eq. (5.1) above Goal Efficient RL fine-tuning for LLMs Stable, critic-free optimization"},{"location":"methods/guided_rpo/","title":"\ud83e\udde9 Guided Reward Policy Optimization (GRPO) \u2014 Structured RL for LLM Fine-Tuning","text":""},{"location":"methods/guided_rpo/#1-overview","title":"1. Overview","text":"<p>Guided Reward Policy Optimization (GRPO) is a reinforcement learning-based algorithm for fine-tuning Large Language Models (LLMs) using structured, guided rewards. It extends preference-based or reward-based methods like DRPO by incorporating richer feedback signals \u2014 human, LLM-generated, or heuristic \u2014 to improve instruction-following, safety, and multi-step reasoning.</p> <p>GRPO combines the stability of offline policy optimization with explicit guidance, helping models learn efficiently while avoiding reward hacking and instability issues common in standard RLHF.</p>"},{"location":"methods/guided_rpo/#2-the-big-picture-from-ppodrpo-to-grpo","title":"2. The Big Picture: From PPO/DRPO to GRPO","text":"Stage PPO DRPO GRPO Reward Scalar, learned from RM Pairwise preferences Guided / structured (scalar, stepwise, multi-dimensional) RL Loop Online, iterative Offline Offline or online KL Penalty Manual, global \u274c Not used \u2705 Adaptive, local Complexity High (policy + value + RM) Moderate Higher (requires guided reward design) Data Efficiency Moderate Uses preference pairs Efficient with structured feedback <p>GRPO can be interpreted as \u201cDRPO with a smarter teacher\u201d, providing stepwise or hierarchical guidance rather than only pairwise comparisons.</p>"},{"location":"methods/guided_rpo/#3-intuitive-understanding","title":"3. Intuitive Understanding","text":"<p>Imagine training an assistant:</p> <ul> <li>PPO: Assistant writes an answer \u2192 scored by a reward model \u2192 RL updates policy.</li> <li>DRPO: Assistant sees preferred vs. rejected responses \u2192 updates policy offline.</li> <li>GRPO: Assistant receives guided signals at each reasoning step, including safety checks, intermediate correctness, or multi-objective rewards, then updates its policy while staying close to the reference model.</li> </ul> <p>Thus, GRPO provides structured supervision, improving sample efficiency and safety without requiring purely online scalar reward exploration.</p>"},{"location":"methods/guided_rpo/#4-training-data-and-setup","title":"4. Training Data and Setup","text":"<p>Each GRPO training example consists of \\((x, y, R_{\\text{guided}})\\)</p> <p>where:</p> <ul> <li>\\(x\\) = prompt or input query</li> <li>\\(y\\) = model response</li> <li>\\(R_{\\text{guided}}\\) = guided reward, potentially multi-dimensional or stepwise (human, LLM, heuristics)</li> </ul> <p>The model learns to maximize expected guided reward, while staying close to a reference policy \\(\\pi_{\\text{ref}}\\) (usually the pretrained SFT model).</p>"},{"location":"methods/guided_rpo/#5-grpo-formulation","title":"5. GRPO Formulation","text":"<p>\ud83d\udcd8 Mathematical Formulation</p>"},{"location":"methods/guided_rpo/#51-objective-function","title":"5.1. Objective Function","text":"<p>GRPO extends offline policy optimization with structured reward guidance and KL regularization:</p> \\[ L_{\\mathrm{GRPO}}(\\theta) = -\\mathbb{E}_{x \\sim D} \\Big[ R_{\\text{guided}}(x, \\pi_\\theta(x)) - \\beta \\text{KL}(\\pi_\\theta(\\cdot|x) \\parallel \\pi_{\\text{ref}}(\\cdot|x)) \\Big] \\] <p>where:</p> <ul> <li>\\(\\pi_\\theta\\) = trainable LLM policy</li> <li>\\(\\pi_{\\text{ref}}\\) = frozen reference model</li> <li>\\(R_{\\text{guided}}\\) = structured reward signal (scalar, vector, or stepwise)</li> <li>\\(\\beta\\) = KL regularization coefficient controlling faithfulness to base model</li> </ul> <p>This objective encourages maximizing expected guided reward while preventing drift from the reference model.</p>"},{"location":"methods/guided_rpo/#52-intuition","title":"5.2. Intuition","text":"<ul> <li>The guided reward can be seen as a rich, informative signal replacing sparse scalar rewards.</li> <li>The KL term serves as an implicit regularizer to prevent catastrophic forgetting or reward hacking.</li> <li>GRPO generalizes DRPO: if (R_{\\text{guided}}) is constructed from pairwise preference comparisons, GRPO reduces to DRPO.</li> </ul>"},{"location":"methods/guided_rpo/#53-implementation-notes-best-practices","title":"5.3. Implementation Notes / Best Practices","text":"<ul> <li>Reference model frozen: Ensure \\(\\pi_{\\text{ref}}\\) does not receive gradients.</li> <li>Reward shaping: Use stepwise or multi-objective rewards for complex reasoning tasks.</li> <li>Numerical stability: Normalize rewards and scale KL penalties to avoid instability.</li> <li>KL regularization: Monitor divergence from \\(\\pi_{\\text{ref}}\\) to prevent drift.</li> <li>Batching: Aggregate rewards over multiple steps when using multi-step guidance._</li> <li>Reward flexibility: Can combine human labels, LLM feedback, or programmatic heuristics.</li> <li>Monitoring: Track reward improvements and KL drift separately for stable training.</li> </ul>"},{"location":"methods/guided_rpo/#54-key-takeaways","title":"5.4. Key Takeaways","text":"<ul> <li>GRPO generalizes DRPO by incorporating structured, guided rewards.</li> <li>KL regularization ensures alignment with base model while learning complex behaviors.</li> <li>Efficient for multi-step reasoning, safety-critical tasks, and long-horizon instructions.</li> <li>Provides a flexible, expressive framework for offline or online RLHF-style optimization.</li> </ul>"},{"location":"methods/guided_rpo/#6-implementation-example-pseudocode","title":"6. Implementation Example (Pseudocode)","text":"<pre><code>log_probs = model.logprobs(batch_inputs, batch_outputs)\nlog_probs_ref = ref_model.logprobs(batch_inputs, batch_outputs)\n\n# Compute KL divergence\nkl = (log_probs_ref - log_probs).mean(dim=-1)\n\n# GRPO loss\nloss = -(guided_rewards - beta * kl).mean()\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"methods/guided_rpo/#7-example-use-cases","title":"7. Example Use Cases","text":"<ol> <li>Multi-step reasoning tasks: math, coding, planning</li> <li>Safety-critical tasks: medical, legal, regulated instructions</li> <li>Long-horizon tasks: document summarization, multi-turn dialogue</li> <li>Complex instruction-following: multi-step workflows, simulations</li> </ol>"},{"location":"methods/guided_rpo/#8-comparison-with-ppo-and-drpo","title":"8. Comparison with PPO and DRPO","text":"Aspect PPO DRPO GRPO Reward Model Learned scalar RM \u274c Not needed Optional or explicit guided signals RL Loop Yes (online) \u274c Offline Offline or online KL Penalty Manual, global \u274c Implicit \u2705 Adaptive, local Training Stability Sensitive More stable Stable with guidance Data Efficiency Moderate Uses preference pairs Efficient with structured guidance Complexity High Moderate Higher due to guided reward design"},{"location":"methods/guided_rpo/#9-limitations-and-challenges","title":"9. Limitations and Challenges","text":"<ul> <li>High-quality guided rewards required \u2014 noisy or biased signals can harm performance.</li> <li>More computationally expensive than DRPO.</li> <li>Designing stepwise or structured rewards requires careful planning.</li> <li>Less standardized tooling compared to PPO or DRPO.</li> </ul>"},{"location":"methods/guided_rpo/#10-summary-table","title":"10. Summary Table","text":"Component Role Example Policy Model (LLM) Learns from guided rewards <code>GPT-3</code>, <code>Llama-2</code> Reference Model Provides baseline probabilities SFT model GRPO Objective Maximizes guided reward + KL penalty Loss formula above \u03b2 Parameter Controls proximity to reference Tuning hyperparameter Goal Align behavior efficiently with complex feedback Stable, guided alignment"},{"location":"methods/ppo/","title":"\ud83e\udde0 PPO and Reward Models in LLM Training","text":""},{"location":"methods/ppo/#1-overview","title":"1. Overview","text":"<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm widely used in fine-tuning Large Language Models (LLMs) under the Reinforcement Learning from Human Feedback (RLHF) framework. It helps bridge the gap between human preferences and LLM outputs by optimizing the model\u2019s responses to align with what humans find helpful, safe, or relevant.</p>"},{"location":"methods/ppo/#2-rlhf-pipeline","title":"2. RLHF Pipeline","text":"<p>RLHF typically consists of three stages:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT)</p> <ul> <li>Train a base LLM on high-quality human demonstration data (prompt\u2013response pairs).</li> </ul> </li> <li> <p>Reward Model (RM) Training</p> <ul> <li>Train a model to assign scalar rewards to outputs based on human preferences.</li> </ul> </li> <li> <p>Reinforcement Learning (PPO)</p> <ul> <li>Fine-tune the policy (SFT model) to maximize predicted rewards from the RM.</li> </ul> </li> </ol> <p>\ud83d\udca1 Intuition: PPO teaches the LLM to generate preferred responses indirectly, using the reward model as scalable feedback.</p>"},{"location":"methods/ppo/#3-why-ppo-instead-of-direct-human-feedback","title":"3. Why PPO instead of Direct Human Feedback?","text":"<p>Direct human labeling for all outputs is impractical and noisy. PPO helps by:</p> <ul> <li>Scaling feedback: Reward models generalize human preferences to unseen outputs.</li> <li>Credit assignment: Uses value function and advantage to propagate sequence-level rewards to tokens.</li> <li>Stable updates: Ensures the model does not deviate too far from its original behavior.</li> </ul>"},{"location":"methods/ppo/#4-ppo-key-concepts","title":"4. PPO Key Concepts","text":""},{"location":"methods/ppo/#41-components","title":"4.1 Components","text":"Component Description Policy Model (\u03c0_\u03b8) The trainable LLM generating responses. Reward Model (R_\u03d5) Evaluates outputs, providing scalar rewards. Reference Model (\u03c0_\u03b8_old) Snapshot of policy before update, used for stable updates. Value Function (V_\u03b8) Estimates expected reward for a given prompt. Advantage (A_t) Measures how much better an action is than expected: <code>A = R - V_\u03b8(s)</code>"},{"location":"methods/ppo/#42-intuition","title":"4.2 Intuition","text":"<p>PPO adjusts the LLM to improve rewards without drastic changes:</p> <ul> <li>Generates outputs \u2192 reward model evaluates \u2192 advantage guides update.</li> <li>The clipped objective prevents extreme updates and maintains stability.</li> </ul>"},{"location":"methods/ppo/#5-ppo-objective-function","title":"5. PPO Objective Function","text":"<p>The Proximal Policy Optimization (PPO) algorithm optimizes a policy model \\(\u03c0_\u03b8\\) while constraining how much it can diverge from a reference (old) policy \\(\u03c0_{\u03b8_{old}}\\).</p> <p>\ud83d\udcd8 PPO Mathematical Formulation</p> <p>For computing \\(A_t\\) (advantage), \\(V_\u03b8(s_t)\\) (value), and rewards, refer to the next section on Value Function and Reward Computation.</p>"},{"location":"methods/ppo/#51-probability-ratio","title":"5.1. Probability Ratio","text":"\\[ r_t(\\theta) = \\frac{\u03c0_\u03b8(a_t | s_t)}{\u03c0_{\u03b8_{old}}(a_t | s_t)} \\] <p>The ratio measures how much the new policy\u2019s likelihood of an action changes compared to the old policy. This ratio quantifies the magnitude and direction of policy change for each sampled token or action.</p>"},{"location":"methods/ppo/#52-clipped-ppo-objective","title":"5.2. Clipped PPO Objective","text":"<p>The clipped surrogate loss ensures stable updates by penalizing large deviations in \\(r_t(\\theta)\\):</p> \\[ L^{PPO}(\\theta) = \\mathbb{E}_t \\left[\\min\\left(r_t(\\theta) A_t,\\ \\text{clip}(r_t(\\theta),\\ 1-\\epsilon,\\ 1+\\epsilon)\\ A_t\\right)\\right] \\] <p>Where:</p> <ul> <li>\\(A_t\\): Advantage function \u2014 how much better an action is than expected.  </li> <li>\\(\u03b5\\): Clipping threshold (typically 0.1\u20130.3).  </li> <li>The <code>min</code> operation limits large, destabilizing updates.</li> </ul>"},{"location":"methods/ppo/#6-value-function-advantage-and-reward-computation","title":"6. Value Function, Advantage, and Reward Computation","text":"<p>The PPO algorithm relies on several auxiliary components \u2014 value function, advantage estimation, and entropy regularization \u2014 that ensure stable and meaningful policy updates.</p> <p>\ud83d\udcd7 Supporting Mathematical Components</p>"},{"location":"methods/ppo/#61-cumulative-reward-return","title":"6.1. Cumulative Reward (Return)","text":"<p>The cumulative reward (or return) represents the total discounted reward starting from time \\(t\\):</p> \\[ R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\] <ul> <li>\\(r_t\\): reward received at time \\(t\\) (from the reward model in RLHF).  </li> <li>\\(\\gamma\\): discount factor (typically 0.95\u20130.99).  </li> </ul> <p>In RLHF, this is often simplified since responses are short (e.g., one reward per sequence).</p> Reward Simplification in RLHF <p>In Reinforcement Learning from Human Feedback (RLHF) \u2014 especially in language model fine-tuning \u2014 the setup is simplified because responses are short and discrete.</p> <ul> <li>A prompt acts as the state \\( s \\).  </li> <li>The model's response (a sequence of tokens) is treated as the action \\( a \\).  </li> <li>A reward model (RM) assigns a single scalar reward \\( r(s, a) \\) for the entire sequence, not per token.</li> </ul> <p>Therefore: $$ R = r(s, a) $$</p> <p>The advantage and value function are computed at the sequence level rather than stepwise. This eliminates the need to sum discounted rewards across timesteps \u2014 simplifying PPO training in RLHF. The loss functions remain structurally similar but are applied to sequence-level rewards.</p>"},{"location":"methods/ppo/#62-value-function","title":"6.2. Value Function","text":"<p>The value function estimates the expected return given a state (or prompt context):</p> \\[ V_\\theta(s_t) \\approx \\mathbb{E}[R_t \\mid s_t] \\] <p>The value loss penalizes inaccurate predictions:</p> \\[ L^{value}(\\theta) = \\frac{1}{2} \\left(V_\u03b8(s_t) - R_t\\right)^2 \\] <p>This helps the model learn accurate value estimates for better advantage computation.</p> Value Function in Practice <p>In practice, the value function is implemented as a learned neural network head attached to the policy model. During training:</p> <ol> <li>The environment (or reward model, in RLHF) provides rewards \\( r_t \\) for each step or sequence.  </li> <li>The cumulative discounted reward \\( R_t = \\sum_k \\gamma^k r_{t+k} \\) is computed for each state \\( s_t \\).  </li> <li>The network learns to predict \\( V_\u03b8(s_t) \\) such that it matches the observed return \\( R_t \\).</li> </ol> <p>There are two common approaches:</p> <ul> <li>Monte Carlo estimate: directly use full episode returns \\( R_t \\) (common in RLHF since responses are short).  </li> <li>Bootstrapped estimate: use \\( r_t + \\gamma V_\u03b8(s_{t+1}) \\) to reduce variance (used in continuous RL environments).</li> </ul> <p>Over time, the model minimizes: $$ L^{value}(\\theta) = \\frac{1}{2} (V_\u03b8(s_t) - R_t)^2 $$ making \\( V_\u03b8(s_t) \\) a reliable baseline for computing the advantage: $$ A_t = R_t - V_\u03b8(s_t) $$</p>"},{"location":"methods/ppo/#63-advantage-function","title":"6.3. Advantage Function","text":"<p>The advantage quantifies how much better an action \\(a_t\\) was compared to the expected baseline:</p> \\[ A_t = R_t - V_\u03b8(s_t) \\] <p>In practice, PPO often uses Generalized Advantage Estimation (GAE) for smoother and lower-variance estimates:</p> \\[ A_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>where \\(\\delta_t = r_t + \\gamma V_\u03b8(s_{t+1}) - V_\u03b8(s_t)\\), and \\(\\lambda\\) is the GAE smoothing factor (typically 0.9\u20130.97).</p> Advantage in Practice for LLMs <p>In LLM fine-tuning with PPO, the advantage is typically computed at the sequence level rather than per-token, since the reward model provides a single scalar reward for the entire generated response.</p>"},{"location":"methods/ppo/#practical-computation-steps","title":"\ud83e\udde9 Practical Computation Steps","text":"<ol> <li>For each prompt \\(s\\), the model generates a sequence \\(a = (a_1, a_2, ..., a_T)\\).  </li> <li>The reward model provides a scalar reward \\(r(s, a)\\) for the whole sequence.  </li> <li>The value head of the policy predicts \\(V_\u03b8(s)\\), estimating the expected reward before generation.  </li> <li>The advantage is then computed as:    $$    A = r(s, a) - V_\u03b8(s)    $$    representing how much better or worse the actual outcome was compared to the model\u2019s expectation.</li> </ol>"},{"location":"methods/ppo/#when-token-level-advantages-are-used","title":"\ud83e\uddee When Token-Level Advantages Are Used","text":"<ul> <li>Some PPO implementations for LLMs compute token-level advantages to better attribute credit across the generated sequence.  </li> <li>This is achieved by assigning the same scalar reward to all tokens in a sequence and using GAE to smooth the signal:   $$   A_t = \\text{GAE}(r_t, V_\u03b8(s_t))   $$</li> <li>This provides more stable gradients and allows finer control during backpropagation.</li> </ul>"},{"location":"methods/ppo/#summary","title":"\u2696\ufe0f Summary","text":"<ul> <li>Sequence-level PPO (common in RLHF): \\(A = r(s, a) - V_\u03b8(s)\\)   \u2192 simpler and effective when rewards are sparse (one per output).</li> <li>Token-level PPO (advanced setups):   Uses GAE to propagate reward information across tokens, reducing variance in updates.</li> </ul> <p>Overall, the advantage serves as the direction and strength of the policy gradient update \u2014 guiding PPO to reinforce actions that outperform the model\u2019s baseline expectations.</p>"},{"location":"methods/ppo/#64-entropy-bonus-exploration-term","title":"6.4. Entropy Bonus (Exploration Term)","text":"<p>The entropy loss encourages the policy to explore rather than prematurely converge:</p> \\[ L^{entropy}(\\theta) = - \\sum_a \u03c0_\u03b8(a|s_t) \\log \u03c0_\u03b8(a|s_t) \\] <p>Higher entropy = more exploration and diversity in generated responses.</p>"},{"location":"methods/ppo/#65-combined-ppo-loss","title":"6.5. Combined PPO Loss","text":"<p>The full training objective combines all three components \u2014 PPO loss, value loss, and entropy term:</p> \\[ L_{total}(\\theta) = -L^{PPO}(\\theta) + c_1 \\cdot L^{value}(\\theta) - c_2 \\cdot H[\u03c0_\u03b8] \\] <p>Where:</p> <ul> <li>\\(H[\u03c0_\u03b8]\\): entropy term promoting exploration.  </li> <li>\\(c_1, c_2\\): coefficients controlling relative weighting of the losses.  </li> </ul> <p>This total loss balances policy improvement, value estimation accuracy, and exploration.</p>"},{"location":"methods/ppo/#7-iterative-ppo-update","title":"7. Iterative PPO Update","text":"<ol> <li>Generate response with policy model.</li> <li>Compute reward using reward model.</li> <li>Compute log probabilities (old vs new policy).</li> <li>Estimate value using value head.</li> <li>Compute advantage.</li> <li>Update policy using clipped surrogate loss.</li> <li>Update value function.</li> <li>Apply entropy bonus.</li> <li>Update reference model for next iteration.</li> </ol> <p>\u2705 Intuition: PPO updates only when new behavior is better and within a controlled region.</p>"},{"location":"methods/ppo/#8-implementation-example-pseudocode","title":"8. Implementation Example (Pseudocode)","text":"<pre><code>for prompt in prompts:\n    # 1. Generate response\n    response = policy_model.generate(prompt)\n\n    # 2. Compute reward from reward model (sequence-level reward)\n    reward = reward_model(prompt, response)\n\n    # 3. Compute log probabilities from old and new policies\n    logprobs_old = ref_model.logprobs(prompt, response)\n    logprobs_new = policy_model.logprobs(prompt, response)\n\n    # 4. Compute value estimate from value head\n    value = value_head(prompt)  # V_theta(s)\n\n    # 5. Compute advantage\n    advantage = reward - value  # sequence-level advantage\n    # optionally: use GAE for token-level advantages\n\n    # 6. Compute PPO ratio and clipped surrogate loss\n    ratio = torch.exp(logprobs_new - logprobs_old)\n    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n    policy_loss = -torch.mean(torch.min(ratio * advantage, clipped_ratio * advantage))\n\n    # 7. Compute value loss\n    value_loss = 0.5 * (value - reward) ** 2\n\n    # 8. Compute entropy bonus for exploration\n    entropy = -torch.sum(torch.exp(logprobs_new) * logprobs_new)\n    entropy_coeff = 0.01  # example weight\n\n    # 9. Combine losses\n    total_loss = policy_loss + 0.5 * value_loss - entropy_coeff * entropy\n\n    # 10. Backpropagate and update model\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    # 11. Update reference model for next iteration\n    ref_model.load_state_dict(policy_model.state_dict())\n</code></pre>"},{"location":"methods/ppo/#9-limitations-and-challenges-of-ppo-in-llm-training","title":"9. Limitations and Challenges of PPO in LLM Training","text":""},{"location":"methods/ppo/#1-kl-divergence-sensitivity","title":"\ud83e\udde9 1. KL Divergence Sensitivity","text":"<p>PPO adds a KL penalty to prevent the model from drifting too far: $$ L = L^{PPO} - \\beta D_{KL}(\\pi_{\\theta} || \\pi_{\\text{ref}}) $$</p> <pre><code>- **Too small \u03b2:** model diverges.\n- **Too large \u03b2:** slow learning.\n- Adaptive KL control helps adjust automatically.\n</code></pre>"},{"location":"methods/ppo/#2-high-training-cost","title":"\u23f3 2. High Training Cost","text":"<ul> <li>Multiple models (policy, reference, reward, value) increase compute.</li> <li>Fine-tuning large LLMs can require thousands of GPU-hours.</li> </ul>"},{"location":"methods/ppo/#3-reward-hacking","title":"\u26a0\ufe0f 3. Reward Hacking","text":"<ul> <li>LLM may over-optimize for the reward model instead of true human preference.</li> <li>Can result in overly polite, verbose, or misleading responses.</li> </ul>"},{"location":"methods/ppo/#4-sparse-or-noisy-rewards","title":"\ud83e\uddee 4. Sparse or Noisy Rewards","text":"<ul> <li>Sparse: One reward per sequence makes credit assignment harder.</li> <li>Noisy: Subjective or inconsistent human preferences can lead to unstable updates. <p>\ud83d\udca1 Sparse/noisy rewards increase variance and slow learning.</p> </li> </ul>"},{"location":"methods/ppo/#5-credit-assignment","title":"\ud83d\udd01 5. Credit Assignment","text":"<ul> <li>Per-token updates but per-sequence rewards create ambiguity about which tokens contributed most.</li> </ul>"},{"location":"methods/ppo/#6-exploration-vs-alignment","title":"\u2696\ufe0f 6. Exploration vs Alignment","text":"<ul> <li>Encouraging exploration may generate unsafe outputs.</li> <li>Balancing diversity and alignment is challenging.</li> </ul>"},{"location":"methods/ppo/#7-implementation-complexity","title":"\ud83d\udd0d 7. Implementation Complexity","text":"<ul> <li>Multiple models and careful hyperparameter tuning required.</li> <li>Can be unstable if any component is suboptimal.</li> </ul>"},{"location":"methods/ppo/#10-summary","title":"10. Summary","text":"Component Role Example Policy Model (LLM) Generates responses to prompts <code>GPT-3</code>, <code>Llama-2</code> Reward Model Scores how much humans like the output Fine-tuned classifier PPO Algorithm Updates the policy using rewards Training loop KL Penalty Prevents over-deviation from base model Regularization Goal Align LLM behavior with human intent Helpful, safe, and truthful answers"}]}