
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Technical documentation and notes on RL methods for LLM fine-tuning">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../dpo/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Proximal Policy Optimization (PPO) - LLM RL Fine-tuning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ppo-and-reward-models-in-llm-training" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM RL Fine-tuning" class="md-header__button md-logo" aria-label="LLM RL Fine-tuning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM RL Fine-tuning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Proximal Policy Optimization (PPO)
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM RL Fine-tuning" class="md-nav__button md-logo" aria-label="LLM RL Fine-tuning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLM RL Fine-tuning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Proximal Policy Optimization (PPO)
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Proximal Policy Optimization (PPO)
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-rlhf-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      2. RLHF Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-why-ppo-instead-of-direct-human-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      3. Why PPO instead of Direct Human Feedback?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-ppo-key-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      4. PPO Key Concepts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. PPO Key Concepts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-components" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Intuition
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-ppo-objective-function" class="md-nav__link">
    <span class="md-ellipsis">
      5. PPO Objective Function
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. PPO Objective Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-probability-ratio" class="md-nav__link">
    <span class="md-ellipsis">
      5.1. Probability Ratio
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-clipped-ppo-objective" class="md-nav__link">
    <span class="md-ellipsis">
      5.2. Clipped PPO Objective
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-value-function-advantage-and-reward-computation" class="md-nav__link">
    <span class="md-ellipsis">
      6. Value Function, Advantage, and Reward Computation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Value Function, Advantage, and Reward Computation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-cumulative-reward-return" class="md-nav__link">
    <span class="md-ellipsis">
      6.1. Cumulative Reward (Return)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      6.2. Value Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-advantage-function" class="md-nav__link">
    <span class="md-ellipsis">
      6.3. Advantage Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-computation-steps" class="md-nav__link">
    <span class="md-ellipsis">
      🧩 Practical Computation Steps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-token-level-advantages-are-used" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 When Token-Level Advantages Are Used
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      ⚖️ Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64-entropy-bonus-exploration-term" class="md-nav__link">
    <span class="md-ellipsis">
      6.4. Entropy Bonus (Exploration Term)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#65-combined-ppo-loss" class="md-nav__link">
    <span class="md-ellipsis">
      6.5. Combined PPO Loss
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-iterative-ppo-update" class="md-nav__link">
    <span class="md-ellipsis">
      7. Iterative PPO Update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-implementation-example-pseudocode" class="md-nav__link">
    <span class="md-ellipsis">
      8. Implementation Example (Pseudocode)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-limitations-and-challenges-of-ppo-in-llm-training" class="md-nav__link">
    <span class="md-ellipsis">
      9. Limitations and Challenges of PPO in LLM Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Limitations and Challenges of PPO in LLM Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-kl-divergence-sensitivity" class="md-nav__link">
    <span class="md-ellipsis">
      🧩 1. KL Divergence Sensitivity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-high-training-cost" class="md-nav__link">
    <span class="md-ellipsis">
      ⏳ 2. High Training Cost
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-reward-hacking" class="md-nav__link">
    <span class="md-ellipsis">
      ⚠️ 3. Reward Hacking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-sparse-or-noisy-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 4. Sparse or Noisy Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-credit-assignment" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 5. Credit Assignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-exploration-vs-alignment" class="md-nav__link">
    <span class="md-ellipsis">
      ⚖️ 6. Exploration vs Alignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-implementation-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 7. Implementation Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-summary" class="md-nav__link">
    <span class="md-ellipsis">
      10. Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Direct Policy Optimization (DPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../drpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Direct Rewards Policy Optimization (DRPO Offline)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decoupled_rpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Decoupled Rewards Policy Optimization (DRPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guided_rpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Guided Rewards Policy Optimization (GRPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grouped_relative_po_deepseek/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Grouped Relative Policy Optimization (Deepseek GRPO)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Advanced Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced_topics/kl_penalty/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KL Penalty
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced_topics/deepseek_rl_finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deepseek RL Fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced_topics/rewards_hacking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Rewards Hacking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-rlhf-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      2. RLHF Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-why-ppo-instead-of-direct-human-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      3. Why PPO instead of Direct Human Feedback?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-ppo-key-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      4. PPO Key Concepts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. PPO Key Concepts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-components" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Intuition
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-ppo-objective-function" class="md-nav__link">
    <span class="md-ellipsis">
      5. PPO Objective Function
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. PPO Objective Function">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-probability-ratio" class="md-nav__link">
    <span class="md-ellipsis">
      5.1. Probability Ratio
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#52-clipped-ppo-objective" class="md-nav__link">
    <span class="md-ellipsis">
      5.2. Clipped PPO Objective
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-value-function-advantage-and-reward-computation" class="md-nav__link">
    <span class="md-ellipsis">
      6. Value Function, Advantage, and Reward Computation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Value Function, Advantage, and Reward Computation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-cumulative-reward-return" class="md-nav__link">
    <span class="md-ellipsis">
      6.1. Cumulative Reward (Return)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      6.2. Value Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-advantage-function" class="md-nav__link">
    <span class="md-ellipsis">
      6.3. Advantage Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-computation-steps" class="md-nav__link">
    <span class="md-ellipsis">
      🧩 Practical Computation Steps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-token-level-advantages-are-used" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 When Token-Level Advantages Are Used
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      ⚖️ Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64-entropy-bonus-exploration-term" class="md-nav__link">
    <span class="md-ellipsis">
      6.4. Entropy Bonus (Exploration Term)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#65-combined-ppo-loss" class="md-nav__link">
    <span class="md-ellipsis">
      6.5. Combined PPO Loss
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-iterative-ppo-update" class="md-nav__link">
    <span class="md-ellipsis">
      7. Iterative PPO Update
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-implementation-example-pseudocode" class="md-nav__link">
    <span class="md-ellipsis">
      8. Implementation Example (Pseudocode)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-limitations-and-challenges-of-ppo-in-llm-training" class="md-nav__link">
    <span class="md-ellipsis">
      9. Limitations and Challenges of PPO in LLM Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Limitations and Challenges of PPO in LLM Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-kl-divergence-sensitivity" class="md-nav__link">
    <span class="md-ellipsis">
      🧩 1. KL Divergence Sensitivity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-high-training-cost" class="md-nav__link">
    <span class="md-ellipsis">
      ⏳ 2. High Training Cost
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-reward-hacking" class="md-nav__link">
    <span class="md-ellipsis">
      ⚠️ 3. Reward Hacking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-sparse-or-noisy-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 4. Sparse or Noisy Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-credit-assignment" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 5. Credit Assignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-exploration-vs-alignment" class="md-nav__link">
    <span class="md-ellipsis">
      ⚖️ 6. Exploration vs Alignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-implementation-complexity" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 7. Implementation Complexity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-summary" class="md-nav__link">
    <span class="md-ellipsis">
      10. Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="ppo-and-reward-models-in-llm-training">🧠 PPO and Reward Models in LLM Training<a class="headerlink" href="#ppo-and-reward-models-in-llm-training" title="Permanent link">&para;</a></h1>
<h3 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h3>
<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm widely used in <strong>fine-tuning Large Language Models (LLMs)</strong> under the Reinforcement Learning from Human Feedback (RLHF) framework. It helps bridge the gap between <strong>human preferences</strong> and <strong>LLM outputs</strong> by optimizing the model’s responses to align with what humans find helpful, safe, or relevant.</p>
<hr />
<h3 id="2-rlhf-pipeline">2. RLHF Pipeline<a class="headerlink" href="#2-rlhf-pipeline" title="Permanent link">&para;</a></h3>
<p>RLHF typically consists of three stages:</p>
<ol>
<li>
<p><strong>Supervised Fine-Tuning (SFT)</strong></p>
<ul>
<li>Train a base LLM on high-quality human demonstration data (prompt–response pairs).</li>
</ul>
</li>
<li>
<p><strong>Reward Model (RM) Training</strong></p>
<ul>
<li>Train a model to assign <strong>scalar rewards</strong> to outputs based on human preferences.</li>
</ul>
</li>
<li>
<p><strong>Reinforcement Learning (PPO)</strong></p>
<ul>
<li>Fine-tune the policy (SFT model) to maximize predicted rewards from the RM.</li>
</ul>
</li>
</ol>
<blockquote>
<p>💡 <strong>Intuition:</strong> PPO teaches the LLM to generate preferred responses indirectly, using the reward model as scalable feedback.</p>
</blockquote>
<hr />
<h3 id="3-why-ppo-instead-of-direct-human-feedback">3. Why PPO instead of Direct Human Feedback?<a class="headerlink" href="#3-why-ppo-instead-of-direct-human-feedback" title="Permanent link">&para;</a></h3>
<p>Direct human labeling for all outputs is <strong>impractical and noisy</strong>. PPO helps by:</p>
<ul>
<li><strong>Scaling feedback:</strong> Reward models generalize human preferences to unseen outputs.</li>
<li><strong>Credit assignment:</strong> Uses value function and advantage to propagate sequence-level rewards to tokens.</li>
<li><strong>Stable updates:</strong> Ensures the model does not deviate too far from its original behavior.</li>
</ul>
<hr />
<h3 id="4-ppo-key-concepts">4. PPO Key Concepts<a class="headerlink" href="#4-ppo-key-concepts" title="Permanent link">&para;</a></h3>
<h4 id="41-components">4.1 Components<a class="headerlink" href="#41-components" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Policy Model (π_θ)</strong></td>
<td>The trainable LLM generating responses.</td>
</tr>
<tr>
<td><strong>Reward Model (R_ϕ)</strong></td>
<td>Evaluates outputs, providing scalar rewards.</td>
</tr>
<tr>
<td><strong>Reference Model (π_θ_old)</strong></td>
<td>Snapshot of policy before update, used for stable updates.</td>
</tr>
<tr>
<td><strong>Value Function (V_θ)</strong></td>
<td>Estimates expected reward for a given prompt.</td>
</tr>
<tr>
<td><strong>Advantage (A_t)</strong></td>
<td>Measures how much better an action is than expected: <code>A = R - V_θ(s)</code></td>
</tr>
</tbody>
</table>
<h4 id="42-intuition">4.2 Intuition<a class="headerlink" href="#42-intuition" title="Permanent link">&para;</a></h4>
<p>PPO adjusts the LLM to improve rewards <strong>without drastic changes</strong>:</p>
<ul>
<li>Generates outputs → reward model evaluates → advantage guides update.</li>
<li>The <strong>clipped objective</strong> prevents extreme updates and maintains stability.</li>
</ul>
<hr />
<h3 id="5-ppo-objective-function">5. PPO Objective Function<a class="headerlink" href="#5-ppo-objective-function" title="Permanent link">&para;</a></h3>
<p>The <strong>Proximal Policy Optimization (PPO)</strong> algorithm optimizes a policy model <span class="arithmatex">\(π_θ\)</span> while constraining how much it can diverge from a reference (old) policy <span class="arithmatex">\(π_{θ_{old}}\)</span>.</p>
<div class="admonition example">
<p class="admonition-title">📘 PPO Mathematical Formulation</p>
<h4 id="51-probability-ratio">5.1. Probability Ratio<a class="headerlink" href="#51-probability-ratio" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
r_t(\theta) = \frac{π_θ(a_t | s_t)}{π_{θ_{old}}(a_t | s_t)}
\]</div>
<p>The ratio measures how much the new policy’s likelihood of an action changes compared to the old policy.
This ratio quantifies the magnitude and direction of policy change for each sampled token or action.</p>
<hr />
<h3 id="52-clipped-ppo-objective">5.2. Clipped PPO Objective<a class="headerlink" href="#52-clipped-ppo-objective" title="Permanent link">&para;</a></h3>
<p>The clipped surrogate loss ensures stable updates by penalizing large deviations in <span class="arithmatex">\(r_t(\theta)\)</span>:</p>
<div class="arithmatex">\[
L^{PPO}(\theta) = \mathbb{E}_t \left[\min\left(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),\ 1-\epsilon,\ 1+\epsilon)\ A_t\right)\right]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(A_t\)</span>: <strong>Advantage function</strong> — how much better an action is than expected.  </li>
<li><span class="arithmatex">\(ε\)</span>: <strong>Clipping threshold</strong> (typically 0.1–0.3).  </li>
<li>The <code>min</code> operation limits large, destabilizing updates.</li>
</ul>
</div>
<blockquote>
<p>For computing <span class="arithmatex">\(A_t\)</span> (advantage), <span class="arithmatex">\(V_θ(s_t)\)</span> (value), and rewards, refer to the next section on <em>Value Function and Reward Computation.</em></p>
</blockquote>
<hr />
<h3 id="6-value-function-advantage-and-reward-computation">6. Value Function, Advantage, and Reward Computation<a class="headerlink" href="#6-value-function-advantage-and-reward-computation" title="Permanent link">&para;</a></h3>
<p>The PPO algorithm relies on several auxiliary components — <strong>value function</strong>, <strong>advantage estimation</strong>, and <strong>entropy regularization</strong> — that ensure stable and meaningful policy updates.</p>
<div class="admonition example">
<p class="admonition-title">📗 Supporting Mathematical Components</p>
<h4 id="61-cumulative-reward-return">6.1. Cumulative Reward (Return)<a class="headerlink" href="#61-cumulative-reward-return" title="Permanent link">&para;</a></h4>
<p>The <strong>cumulative reward</strong> (or <em>return</em>) represents the total discounted reward starting from time <span class="arithmatex">\(t\)</span>:</p>
<div class="arithmatex">\[
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
\]</div>
<ul>
<li><span class="arithmatex">\(r_t\)</span>: reward received at time <span class="arithmatex">\(t\)</span> (from the reward model in RLHF).  </li>
<li><span class="arithmatex">\(\gamma\)</span>: discount factor (typically 0.95–0.99).  </li>
</ul>
<p>In RLHF, this is often simplified since responses are short (e.g., one reward per sequence).</p>
<details class="info">
<summary>Reward Simplification in RLHF</summary>
<p>In <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> — especially in <strong>language model fine-tuning</strong> — the setup is simplified because responses are short and discrete.</p>
<ul>
<li>A <strong>prompt</strong> acts as the state <span class="arithmatex">\( s \)</span>.  </li>
<li>The <strong>model's response</strong> (a sequence of tokens) is treated as the action <span class="arithmatex">\( a \)</span>.  </li>
<li>A <strong>reward model (RM)</strong> assigns <strong>a single scalar reward</strong> <span class="arithmatex">\( r(s, a) \)</span> for the <em>entire sequence</em>, not per token.</li>
</ul>
<p>Therefore:
$$
R = r(s, a)
$$</p>
<p>The <strong>advantage</strong> and <strong>value function</strong> are computed at the sequence level rather than stepwise.<br />
This eliminates the need to sum discounted rewards across timesteps — simplifying PPO training in RLHF.<br />
The loss functions remain structurally similar but are applied to <strong>sequence-level rewards</strong>.</p>
</details>
<hr />
<h4 id="62-value-function">6.2. Value Function<a class="headerlink" href="#62-value-function" title="Permanent link">&para;</a></h4>
<p>The <strong>value function</strong> estimates the expected return given a state (or prompt context):</p>
<div class="arithmatex">\[
V_\theta(s_t) \approx \mathbb{E}[R_t \mid s_t]
\]</div>
<p>The <strong>value loss</strong> penalizes inaccurate predictions:</p>
<div class="arithmatex">\[
L^{value}(\theta) = \frac{1}{2} \left(V_θ(s_t) - R_t\right)^2
\]</div>
<p>This helps the model learn accurate value estimates for better advantage computation.</p>
<details class="info">
<summary>Value Function in Practice</summary>
<p>In practice, the <strong>value function</strong> is implemented as a <strong>learned neural network head</strong> attached to the policy model.<br />
During training:</p>
<ol>
<li>The environment (or reward model, in RLHF) provides rewards <span class="arithmatex">\( r_t \)</span> for each step or sequence.  </li>
<li>The <strong>cumulative discounted reward</strong> <span class="arithmatex">\( R_t = \sum_k \gamma^k r_{t+k} \)</span> is computed for each state <span class="arithmatex">\( s_t \)</span>.  </li>
<li>The network learns to predict <span class="arithmatex">\( V_θ(s_t) \)</span> such that it <strong>matches the observed return</strong> <span class="arithmatex">\( R_t \)</span>.</li>
</ol>
<p>There are two common approaches:</p>
<ul>
<li><strong>Monte Carlo estimate:</strong> directly use full episode returns <span class="arithmatex">\( R_t \)</span> (common in RLHF since responses are short).  </li>
<li><strong>Bootstrapped estimate:</strong> use <span class="arithmatex">\( r_t + \gamma V_θ(s_{t+1}) \)</span> to reduce variance (used in continuous RL environments).</li>
</ul>
<p>Over time, the model minimizes:
$$
L^{value}(\theta) = \frac{1}{2} (V_θ(s_t) - R_t)^2
$$
making <span class="arithmatex">\( V_θ(s_t) \)</span> a reliable <strong>baseline</strong> for computing the advantage:
$$
A_t = R_t - V_θ(s_t)
$$</p>
</details>
<hr />
<h4 id="63-advantage-function">6.3. Advantage Function<a class="headerlink" href="#63-advantage-function" title="Permanent link">&para;</a></h4>
<p>The <strong>advantage</strong> quantifies how much better an action <span class="arithmatex">\(a_t\)</span> was compared to the expected baseline:</p>
<div class="arithmatex">\[
A_t = R_t - V_θ(s_t)
\]</div>
<p>In practice, PPO often uses <strong>Generalized Advantage Estimation (GAE)</strong> for smoother and lower-variance estimates:</p>
<div class="arithmatex">\[
A_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\]</div>
<p>where<br />
<span class="arithmatex">\(\delta_t = r_t + \gamma V_θ(s_{t+1}) - V_θ(s_t)\)</span>,<br />
and <span class="arithmatex">\(\lambda\)</span> is the <em>GAE smoothing factor</em> (typically 0.9–0.97).</p>
<details class="info">
<summary>Advantage in Practice for LLMs</summary>
<p>In <strong>LLM fine-tuning with PPO</strong>, the <strong>advantage</strong> is typically computed at the <strong>sequence level</strong> rather than per-token, since the reward model provides a <strong>single scalar reward</strong> for the entire generated response.</p>
<h4 id="practical-computation-steps">🧩 Practical Computation Steps<a class="headerlink" href="#practical-computation-steps" title="Permanent link">&para;</a></h4>
<ol>
<li>For each prompt <span class="arithmatex">\(s\)</span>, the model generates a sequence <span class="arithmatex">\(a = (a_1, a_2, ..., a_T)\)</span>.  </li>
<li>The <strong>reward model</strong> provides a scalar reward <span class="arithmatex">\(r(s, a)\)</span> for the whole sequence.  </li>
<li>The <strong>value head</strong> of the policy predicts <span class="arithmatex">\(V_θ(s)\)</span>, estimating the expected reward before generation.  </li>
<li>The <strong>advantage</strong> is then computed as:
   $$
   A = r(s, a) - V_θ(s)
   $$
   representing how much better or worse the actual outcome was compared to the model’s expectation.</li>
</ol>
<h4 id="when-token-level-advantages-are-used">🧮 When Token-Level Advantages Are Used<a class="headerlink" href="#when-token-level-advantages-are-used" title="Permanent link">&para;</a></h4>
<ul>
<li>Some PPO implementations for LLMs compute <strong>token-level advantages</strong> to better attribute credit across the generated sequence.  </li>
<li>This is achieved by assigning the same scalar reward to all tokens in a sequence and using GAE to smooth the signal:
  $$
  A_t = \text{GAE}(r_t, V_θ(s_t))
  $$</li>
<li>This provides more stable gradients and allows finer control during backpropagation.</li>
</ul>
<h4 id="summary">⚖️ Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Sequence-level PPO (common in RLHF):</strong><br />
<span class="arithmatex">\(A = r(s, a) - V_θ(s)\)</span><br />
  → simpler and effective when rewards are sparse (one per output).</li>
<li><strong>Token-level PPO (advanced setups):</strong><br />
  Uses GAE to propagate reward information across tokens, reducing variance in updates.</li>
</ul>
<p>Overall, the advantage serves as the <strong>direction and strength</strong> of the policy gradient update — guiding PPO to reinforce actions that outperform the model’s baseline expectations.</p>
</details>
<h4 id="64-entropy-bonus-exploration-term">6.4. Entropy Bonus (Exploration Term)<a class="headerlink" href="#64-entropy-bonus-exploration-term" title="Permanent link">&para;</a></h4>
<p>The <strong>entropy loss</strong> encourages the policy to explore rather than prematurely converge:</p>
<div class="arithmatex">\[
L^{entropy}(\theta) = - \sum_a π_θ(a|s_t) \log π_θ(a|s_t)
\]</div>
<p>Higher entropy = more exploration and diversity in generated responses.</p>
<hr />
<h4 id="65-combined-ppo-loss">6.5. Combined PPO Loss<a class="headerlink" href="#65-combined-ppo-loss" title="Permanent link">&para;</a></h4>
<p>The full training objective combines all three components — PPO loss, value loss, and entropy term:</p>
<div class="arithmatex">\[
L_{total}(\theta) = -L^{PPO}(\theta) + c_1 \cdot L^{value}(\theta) - c_2 \cdot H[π_θ]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(H[π_θ]\)</span>: entropy term promoting exploration.  </li>
<li><span class="arithmatex">\(c_1, c_2\)</span>: coefficients controlling relative weighting of the losses.  </li>
</ul>
<p>This total loss balances <strong>policy improvement</strong>, <strong>value estimation accuracy</strong>, and <strong>exploration</strong>.</p>
</div>
<h3 id="7-iterative-ppo-update">7. Iterative PPO Update<a class="headerlink" href="#7-iterative-ppo-update" title="Permanent link">&para;</a></h3>
<ol>
<li>Generate response with policy model.</li>
<li>Compute reward using reward model.</li>
<li>Compute log probabilities (old vs new policy).</li>
<li>Estimate value using value head.</li>
<li>Compute advantage.</li>
<li>Update policy using <strong>clipped surrogate loss</strong>.</li>
<li>Update value function.</li>
<li>Apply entropy bonus.</li>
<li>Update reference model for next iteration.</li>
</ol>
<blockquote>
<p>✅ <strong>Intuition:</strong> PPO updates only when new behavior is better and within a controlled region.</p>
</blockquote>
<hr />
<h3 id="8-implementation-example-pseudocode">8. Implementation Example (Pseudocode)<a class="headerlink" href="#8-implementation-example-pseudocode" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="c1"># 1. Generate response</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">policy_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

    <span class="c1"># 2. Compute reward from reward model (sequence-level reward)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>

    <span class="c1"># 3. Compute log probabilities from old and new policies</span>
    <span class="n">logprobs_old</span> <span class="o">=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">logprobs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>
    <span class="n">logprobs_new</span> <span class="o">=</span> <span class="n">policy_model</span><span class="o">.</span><span class="n">logprobs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>

    <span class="c1"># 4. Compute value estimate from value head</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value_head</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>  <span class="c1"># V_theta(s)</span>

    <span class="c1"># 5. Compute advantage</span>
    <span class="n">advantage</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">value</span>  <span class="c1"># sequence-level advantage</span>
    <span class="c1"># optionally: use GAE for token-level advantages</span>

    <span class="c1"># 6. Compute PPO ratio and clipped surrogate loss</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprobs_new</span> <span class="o">-</span> <span class="n">logprobs_old</span><span class="p">)</span>
    <span class="n">clipped_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">,</span> <span class="n">clipped_ratio</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">))</span>

    <span class="c1"># 7. Compute value loss</span>
    <span class="n">value_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">reward</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># 8. Compute entropy bonus for exploration</span>
    <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprobs_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">logprobs_new</span><span class="p">)</span>
    <span class="n">entropy_coeff</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># example weight</span>

    <span class="c1"># 9. Combine losses</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">value_loss</span> <span class="o">-</span> <span class="n">entropy_coeff</span> <span class="o">*</span> <span class="n">entropy</span>

    <span class="c1"># 10. Backpropagate and update model</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 11. Update reference model for next iteration</span>
    <span class="n">ref_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">policy_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</code></pre></div>
<h3 id="9-limitations-and-challenges-of-ppo-in-llm-training">9. Limitations and Challenges of PPO in LLM Training<a class="headerlink" href="#9-limitations-and-challenges-of-ppo-in-llm-training" title="Permanent link">&para;</a></h3>
<h4 id="1-kl-divergence-sensitivity">🧩 1. KL Divergence Sensitivity<a class="headerlink" href="#1-kl-divergence-sensitivity" title="Permanent link">&para;</a></h4>
<p>PPO adds a <strong>KL penalty</strong> to prevent the model from drifting too far:
$$
L = L^{PPO} - \beta D_{KL}(\pi_{\theta} || \pi_{\text{ref}})
$$</p>
<pre><code>- **Too small β:** model diverges.
- **Too large β:** slow learning.
- Adaptive KL control helps adjust automatically.
</code></pre>
<hr />
<h4 id="2-high-training-cost">⏳ 2. High Training Cost<a class="headerlink" href="#2-high-training-cost" title="Permanent link">&para;</a></h4>
<ul>
<li>Multiple models (policy, reference, reward, value) increase compute.</li>
<li>Fine-tuning large LLMs can require <strong>thousands of GPU-hours</strong>.</li>
</ul>
<hr />
<h4 id="3-reward-hacking">⚠️ 3. Reward Hacking<a class="headerlink" href="#3-reward-hacking" title="Permanent link">&para;</a></h4>
<ul>
<li>LLM may over-optimize for the reward model instead of true human preference.</li>
<li>Can result in overly polite, verbose, or misleading responses.</li>
</ul>
<hr />
<h4 id="4-sparse-or-noisy-rewards">🧮 4. Sparse or Noisy Rewards<a class="headerlink" href="#4-sparse-or-noisy-rewards" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Sparse:</strong> One reward per sequence makes credit assignment harder.</li>
<li><strong>Noisy:</strong> Subjective or inconsistent human preferences can lead to unstable updates.<blockquote>
<p>💡 Sparse/noisy rewards increase variance and slow learning.</p>
</blockquote>
</li>
</ul>
<hr />
<h4 id="5-credit-assignment">🔁 5. Credit Assignment<a class="headerlink" href="#5-credit-assignment" title="Permanent link">&para;</a></h4>
<ul>
<li>Per-token updates but per-sequence rewards create ambiguity about which tokens contributed most.</li>
</ul>
<hr />
<h4 id="6-exploration-vs-alignment">⚖️ 6. Exploration vs Alignment<a class="headerlink" href="#6-exploration-vs-alignment" title="Permanent link">&para;</a></h4>
<ul>
<li>Encouraging exploration may generate unsafe outputs.</li>
<li>Balancing diversity and alignment is challenging.</li>
</ul>
<hr />
<h4 id="7-implementation-complexity">🔍 7. Implementation Complexity<a class="headerlink" href="#7-implementation-complexity" title="Permanent link">&para;</a></h4>
<ul>
<li>Multiple models and careful hyperparameter tuning required.</li>
<li>Can be unstable if any component is suboptimal.</li>
</ul>
<h3 id="10-summary">10. Summary<a class="headerlink" href="#10-summary" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Role</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Policy Model (LLM)</strong></td>
<td>Generates responses to prompts</td>
<td><code>GPT-3</code>, <code>Llama-2</code></td>
</tr>
<tr>
<td><strong>Reward Model</strong></td>
<td>Scores how much humans like the output</td>
<td>Fine-tuned classifier</td>
</tr>
<tr>
<td><strong>PPO Algorithm</strong></td>
<td>Updates the policy using rewards</td>
<td>Training loop</td>
</tr>
<tr>
<td><strong>KL Penalty</strong></td>
<td>Prevents over-deviation from base model</td>
<td>Regularization</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>Align LLM behavior with human intent</td>
<td>Helpful, safe, and truthful answers</td>
</tr>
</tbody>
</table>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>