# ðŸ” LOOP: Reinforcement Learning for Long-Horizon Interactive LLM Agents

**Paper:** [Reinforcement Learning for Long-Horizon Interactive LLM Agents from Chen et al. (2025)](https://arxiv.org/pdf/2502.01600)

### 1. Overview
LOOP (*Leave-One-Out PPO*) is a **reinforcement learning algorithm for interactive digital agents (IDAs)** - large language models that perform multi-step tasks involving API calls and stateful environment interactions.  

Key highlights:

- **Memory-efficient:** no separate value network required.
- **Data-efficient:** uses leave-one-out advantage estimation from multiple rollouts per context.
- Designed for **long-horizon tasks**: stable updates token by token with per-token PPO clipping.
- Evaluated in **AppWorld**, a benchmark with multiple APIs and apps.

___

### 2. Problem Statement

They formalize interactive agent training as a **partially observable Markov decision process (POMDP)**:

- Agent observes a **context** \(c\) (task description).
- Generates a **sequence of tokens** \(x_{1:T}\) (natural language or structured API calls).
- Interacts with environment, yielding a trajectory \(\tau = (c, x_{1:T}, \text{env traces})\).
- Receives a **reward** \(R(\tau)\) at the end of the task or at intermediate steps.

Objective:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
$$

where \(\pi_\theta\) is the autoregressive LLM policy.

---

### 3. Components and Terminology

- **Policy (\(\pi_\theta\))**: autoregressive LLM generating tokens step by step.
- **Rollout (\(\tau\))**: sequence of generated tokens and environment traces for a context.
- **Return (G)**: total episodic reward for a rollout.
- **Leave-One-Out (LOO) Baseline**: average return of all other rollouts for the same context, used to compute advantage.
- **Advantage (A)**: difference between rollout return and LOO baseline.
- **Token-level actions**: each token is treated as a distinct action in the MDP.
- **Importance weights**: used to reuse off-policy rollouts collected under previous policy versions.

---

### 4. LOOP Training Framework

LOOP replaces the standard PPO critic with **LOO baselines** and applies **per-token PPO clipping**:

1. **Collect multiple rollouts per prompt** (\(K\)) using a behavior policy snapshot.
2. **Compute LOO advantage** for each rollout:

    $$
    b^{(i)}_{\text{LOO}} = \frac{1}{K-1}\sum_{k \ne i} G^{(k)}, \quad
    A^{(i)}_{\text{LOO}} = G^{(i)} - b^{(i)}_{\text{LOO}}
    $$

3. **Compute per-token ratios** for current policy vs. behavior policy.
4. **Apply clipped surrogate objective** token by token.
5. **Update policy parameters** with gradient ascent.

**Layman explanation:**  
Instead of training a separate critic, LOOP looks at other rollouts of the same task to see what a â€œtypicalâ€ return is, then nudges the model to favor better-than-average rollouts while limiting the per-token change.

---

### 5. LOOP Objective Function

!!! example "ðŸ“— Mathematical Components"

    For each token \(t\) in rollout \(i\):

    \[
    r_t = \frac{\pi_\theta(x_t \mid c, x_{1:t-1})}{\pi_b(x_t \mid c, x_{1:t-1})}
    \]
    
    Token-level clipped PPO surrogate:
    
    \[
    L_{\text{token}} = \text{mean}_t \Big[ \min(r_t \cdot A^{(i)}_{\text{LOO}}, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon) \cdot A^{(i)}_{\text{LOO}}) \Big]
    \]
    
    Overall loss (to minimize):
    
    \[
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N L_{\text{token}}^{(i)}
    \]

    - \( \pi_\theta \) = This is the main policy we are training, parameterized by $\theta$. Used to generate new rollouts and to compute current token probabilities for gradient updates. Updated every training step using the PPO/LOOP objective.
    - \( \pi_b \) = This is a snapshot of the policy at the time rollouts were collected, also called the behavior policy.
    - \( \varepsilon \) = PPO clip parameter (typically 0.1â€“0.2).
    - \( N \) = number of rollouts in batch.

    > \( \pi_b \) Keeps training stable because old rollouts were generated by a slightly different model.
    Think of it as the â€œteacherâ€ model at collection time â€” it doesnâ€™t change until you refresh the snapshot.


### 6. Implementation Example (Pseudocode)

```python
# LOOP Training Pseudocode

while not converged:
    dataset = []
    for context in batch_contexts:
        rollouts = [sample_rollout(policy, context) for _ in range(K)]
        returns = [get_return(r) for r in rollouts]
        for i in range(K):
            baseline = mean(returns[j] for j in range(K) if j != i)
            advantage = returns[i] - baseline
            dataset.append((context, rollouts[i], advantage))

    # Policy update using token-level clipped PPO
    for epoch in range(EPOCHS):
        for sample in minibatch(dataset):
            logp_old = get_logprobs(Ï€_old, sample)
            logp_new = get_logprobs(Ï€_Î¸, sample)
            r_t = exp(logp_new - logp_old)
            L = mean(min(r_t * A, clip(r_t, 1-Îµ, 1+Îµ) * A))
            loss = -L
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    Ï€_old = copy(Ï€_Î¸)  # update behavior policy snapshot**Layman explanation:**  
```

### 7. Differences from Prior Work

| Feature             | Standard PPO / RLHF | LOOP                                  |
| ------------------- | ------------------- | ------------------------------------- |
| Value network       | Required            | Not required (LOO baseline)           |
| Memory usage        | High                | Low (single LLM)                      |
| Token-level updates | Optional            | Required for stability                |
| Off-policy reuse    | Limited             | Supported via importance weights      |
| Long-horizon tasks  | Challenging         | Improved via LOO + per-token clipping |

> Key insight: LOOP achieves sample and memory efficiency while maintaining PPO-style stability for long sequences.

---


### 8. AppWorld Context

- Environment: 9 apps, 457 API endpoints.
- Tasks: multi-app workflows, up to 40 steps.
- Models evaluated: 32B instruction-tuned LLMs.
- Results: LOOP-trained agents outperform larger closed-source models in API correctness, error recovery, and avoiding unwarranted assumptions.

