# üîÅ LOOP: Reinforcement Learning for Long-Horizon Interactive LLM Agents

**Paper:** [Reinforcement Learning for Long-Horizon Interactive LLM Agents from Chen et al. (2025)](https://arxiv.org/pdf/2502.01600)

### 1. Overview
LOOP (*Leave-One-Out PPO*) is a **reinforcement learning algorithm for interactive digital agents (IDAs)** - large language models that perform multi-step tasks involving API calls and stateful environment interactions.  

Key highlights:

- **Memory-efficient:** no separate value network required.
- **Data-efficient:** uses leave-one-out advantage estimation from multiple rollouts per context.
- Designed for **long-horizon tasks**: stable updates token by token with per-token PPO clipping.
- Evaluated in **AppWorld**, a benchmark with multiple APIs and apps.

___

### 2. Problem Statement

They formalize interactive agent training as a **partially observable Markov decision process (POMDP)**:

- Agent observes a **context** \(c\) (task description).
- Generates a **sequence of tokens** \(x_{1:T}\) (natural language or structured API calls).
- Interacts with environment, yielding a trajectory \(\tau = (c, x_{1:T}, \text{env traces})\).
- Receives a **reward** \(R(\tau)\) at the end of the task or at intermediate steps.

Objective:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
$$

where \(\pi_\theta\) is the autoregressive LLM policy.

---

### 3. Components and Terminology

- **Policy (\(\pi_\theta\))**: autoregressive LLM generating tokens step by step.
- **Rollout (\(\tau\))**: sequence of generated tokens and environment traces for a context.
- **Return (G)**: total episodic reward for a rollout.
- **Leave-One-Out (LOO) Baseline**: average return of all other rollouts for the same context, used to compute advantage.
- **Advantage (A)**: difference between rollout return and LOO baseline.
- **Token-level actions**: each token is treated as a distinct action in the MDP.
- **Importance weights**: used to reuse off-policy rollouts collected under previous policy versions.

---

### 4. LOOP Training Framework

LOOP replaces the standard PPO critic with **LOO baselines** and applies **per-token PPO clipping**:

1. **Collect multiple rollouts per prompt** (\(K\)) using a behavior policy snapshot.
2. **Compute LOO advantage** for each rollout:

    $$
    b^{(i)}_{\text{LOO}} = \frac{1}{K-1}\sum_{k \ne i} G^{(k)}, \quad
    A^{(i)}_{\text{LOO}} = G^{(i)} - b^{(i)}_{\text{LOO}}
    $$

3. **Compute per-token ratios** for current policy vs. behavior policy.
4. **Apply clipped surrogate objective** token by token.
5. **Update policy parameters** with gradient ascent.

**Layman explanation:**  
Instead of training a separate critic, LOOP looks at other rollouts of the same task to see what a ‚Äútypical‚Äù return is, then nudges the model to favor better-than-average rollouts while limiting the per-token change.

---

### 5. LOOP Objective Function

!!! example "üìó Mathematical Components"

    For each token \(t\) in rollout \(i\):

    \[
    r_t = \frac{\pi_\theta(x_t \mid c, x_{1:t-1})}{\pi_b(x_t \mid c, x_{1:t-1})}
    \]
    
    Token-level clipped PPO surrogate:
    
    \[
    L_{\text{token}} = \text{mean}_t \Big[ \min(r_t \cdot A^{(i)}_{\text{LOO}}, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon) \cdot A^{(i)}_{\text{LOO}}) \Big]
    \]
    
    Overall loss (to minimize):
    
    \[
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N L_{\text{token}}^{(i)}
    \]

    - \( \pi_\theta \) = This is the main policy we are training, parameterized by $\theta$. Used to generate new rollouts and to compute current token probabilities for gradient updates. Updated every training step using the PPO/LOOP objective.
    - \( \pi_b \) = This is a snapshot of the policy at the time rollouts were collected, also called the behavior policy.
    - \( \varepsilon \) = PPO clip parameter (typically 0.1‚Äì0.2).
    - \( N \) = number of rollouts in batch.

    > \( \pi_b \) Keeps training stable because old rollouts were generated by a slightly different model.
    Think of it as the ‚Äúteacher‚Äù model at collection time ‚Äî it doesn‚Äôt change until you refresh the snapshot.


### 6. Implementation Example (Pseudocode)

```python
# LOOP Training Pseudocode

while not converged:
    dataset = []
    for context in batch_contexts:
        rollouts = [sample_rollout(policy, context) for _ in range(K)]
        returns = [get_return(r) for r in rollouts]
        for i in range(K):
            baseline = mean(returns[j] for j in range(K) if j != i)
            advantage = returns[i] - baseline
            dataset.append((context, rollouts[i], advantage))

    # Policy update using token-level clipped PPO
    for epoch in range(EPOCHS):
        for sample in minibatch(dataset):
            logp_old = get_logprobs(œÄ_old, sample)
            logp_new = get_logprobs(œÄ_Œ∏, sample)
            r_t = exp(logp_new - logp_old)
            L = mean(min(r_t * A, clip(r_t, 1-Œµ, 1+Œµ) * A))
            loss = -L
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    œÄ_old = copy(œÄ_Œ∏)  # update behavior policy snapshot**Layman explanation:**  
```

### 7. Differences from Prior Work

| Feature             | Standard PPO / RLHF | LOOP                                  |
| ------------------- | ------------------- | ------------------------------------- |
| Value network       | Required            | Not required (LOO baseline)           |
| Memory usage        | High                | Low (single LLM)                      |
| Token-level updates | Optional            | Required for stability                |
| Off-policy reuse    | Limited             | Supported via importance weights      |
| Long-horizon tasks  | Challenging         | Improved via LOO + per-token clipping |

> Key insight: LOOP achieves sample and memory efficiency while maintaining PPO-style stability for long sequences.

---


### 8. AppWorld Context

- Environment: 9 apps, 457 API endpoints.
- Tasks: multi-app workflows, up to 40 steps.
- Models evaluated: 32B instruction-tuned LLMs.
- Results: LOOP-trained agents outperform larger closed-source models in API correctness, error recovery, and avoiding unwarranted assumptions.

### 9. Experimental Setup

#### Train/Dev/Test Splits

| Split          | #Tasks | Difficulty                             |
| -------------- | ------ | -------------------------------------- |
| Train          | 35     | Levels 1‚Äì3                             |
| Dev            | 20     | Levels 1‚Äì3                             |
| Test-Normal    | 20     | Standard tasks                         |
| Test-Challenge | 10     | Longer-horizon, multi-step, edge cases |

#### Evaluation Metrics

1. **Task Goal Completion (TGC)**: Fraction of tasks fully completed successfully.
2. **Scenario Goal Completion (SGC)**: Fraction of subgoals completed across multi-step scenarios.
3. **Behavioral metrics** (analyzed separately):
    - Frequency of consulting API documentation
    - Number of placeholder / dummy values used
    - Frequency of ‚Äúassuming‚Äù or unsupported inferences
    - Number of code cells submitted per turn
    - Recovery from API errors

---

## 10. Quantitative Results

### Main Results: Test Sets

| Method   | Test-N (TGC %) | Test-C (TGC %) |
| -------- | -------------- | -------------- |
| NFT      | 39.1           | 21.5           |
| SFT      | 55.6           | 30.2           |
| DPO      | 58.3           | 32.0           |
| PPO      | 63.1           | 40.5           |
| **LOOP** | **71.3**       | **45.7**       |

**Observations:**

- LOOP achieves the highest TGC on both normal and challenge sets.
- Gains are especially significant on challenging long-horizon tasks.
- Token-level actions + LOO advantage outperform rollout-level PPO variants.

### Scenario Goal Completion (SGC)

- LOOP increases **subgoal completion** by ~15‚Äì20% over SFT.
- Highlights better structured multi-step reasoning.

---

## 11. Qualitative Behavioral Analysis

LOOP-trained agents exhibit distinct behavior changes compared to NFT/SFT baselines:

| Metric                   | NFT/SFT    | LOOP          | Interpretation                                     |
| ------------------------ | ---------- | ------------- | -------------------------------------------------- |
| API doc consultation     | 1x         | 1.6x          | Agents consult docs more often, improving accuracy |
| Placeholder/dummy usage  | high       | 6x reduction  | Less likely to guess missing values                |
| "Assuming" statements    | high       | 30x reduction | Fewer unsupported inferences                       |
| Code cells submitted     | many       | ~6x fewer     | More efficient, fewer unnecessary steps            |
| Recovery from API errors | low        | 3x higher     | Better robustness to failures                      |
| Solution diversity       | 80% unique | 94‚Äì98% unique | Maintains diversity, avoids collapse               |

**Takeaways:**

- LOOP improves **practical reasoning**, **error recovery**, and **efficiency**.
- Reward-based RL training reshapes agent strategies beyond simple completion rate.
- Diversity in solutions suggests better generalization across unseen tasks.

---

## 12. Ablation Studies

### 12.1 Per-Token vs Rollout-Level Advantage

- Per-token advantage (assigning same LOO advantage to each token) outperformed rollout-level or per-turn reward assignment.
- Reason: finer-grained control improves stability in long sequences.

### 12.2 Reward Normalization

- Normalizing trajectory returns (zero-mean, unit-variance) **hurt performance** in this domain.
- Likely because long-horizon tasks have sparse rewards; normalization flattens meaningful differences.

### 12.3 Number of Rollouts per Context (K)

| K | TGC Test-N |
| - | ---------- |
| 2 | 66.2       |
| 4 | 71.3       |
| 6 | 70.8       |

- Small K (<4) underestimates baseline; very large K offers diminishing returns.

**Insights:**

1. **Memory efficiency matters:** single LLM + LOO avoids a separate value network.
2. **Token-level updates** improve stability in autoregressive sequences.
3. **Behavioral metrics reveal true gains**, beyond raw task completion.
4. **LOO baseline** provides low-variance advantage estimates.
5. **Long-horizon RL works**: large LLMs can learn multi-step reasoning when trained carefully with LOOP.
