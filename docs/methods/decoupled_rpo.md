# ðŸ§  Decoupled Reward Policy Optimization (DRPO)

### 1. Overview
Decoupled Reward Policy Optimization (DRPO) is a reinforcement learning algorithm designed to enhance the reasoning capabilities of Large Language Models (LLMs).  
It addresses limitations in traditional reinforcement learning methods by **decoupling the length-based learning signal** of correct rollouts from incorrect ones, preventing penalization of correct but lengthy reasoning processes.  
This is particularly beneficial for tasks requiring complex reasoning and long-form outputs.

---

### 2. DRPO Pipeline
The DRPO framework follows the typical RLHF pipeline with key modifications:

1. **Supervised Fine-Tuning (SFT)**  
    - Train a base LLM on high-quality human demonstration data (promptâ€“response pairs).

2. **Reward Model (RM) Training**  
    - Train a model to assign scalar rewards to outputs based on human preferences.

3. **Reinforcement Learning (DRPO)**  
    - Fine-tune the policy (SFT model) to maximize predicted rewards from the RM, with decoupled reasoning length consideration.

> ðŸ’¡ **Intuition:** DRPO ensures that correct but lengthy responses are appropriately rewarded, allowing the model to generate more complex and accurate outputs.

---

### 3. Why DRPO Instead of Traditional Methods?
- Traditional RL methods can **penalize correct but long reasoning processes** due to group-relative advantage functions.  
- DRPO **decouples the length-based learning signal** of correct rollouts from incorrect ones, ensuring valid reasoning is reinforced.  

---

### 4. DRPO Key Concepts

#### 4.1 Components

| Component             | Description                                           |
|-----------------------|-------------------------------------------------------|
| **Policy Model (Ï€â‚œ)** | The trainable LLM generating responses.             |
| **Reward Model (Râ‚˜)** | Evaluates outputs, providing scalar rewards.         |
| **Reference Model (Ï€â‚œâ‚’â‚—áµˆ)** | Snapshot of policy before update, for stability.  |
| **Value Function (Vâ‚œ)** | Estimates expected reward for a given prompt.      |
| **Advantage (Aâ‚œ)**     | Measures how much better an action is than expected: `Aâ‚œ = Râ‚˜ - Vâ‚œ(sâ‚œ)` |

#### 4.2 Intuition
- Generates outputs â†’ reward model evaluates â†’ advantage guides update.  
- Decoupled objective ensures correct but lengthy reasoning is rewarded.

---

### 5. DRPO Objective Function

!!! example "ðŸ“˜ DRPO Mathematical Formulation" 

    #### 5.1 Probability Ratio
    Measures how much the new policyâ€™s likelihood of an action changes compared to the old policy:
      
    $$
    râ‚œ(Ï€â‚œ) = \frac{Ï€â‚œ(aâ‚œ | sâ‚œ)}{Ï€â‚œâ‚’â‚—áµˆ(aâ‚œ | sâ‚œ)}
    $$

    #### 5.2 Decoupled Surrogate Loss
    Ensures stable updates while rewarding correct reasoning lengths:

    $$
    L^{DRPO}(Ï€â‚œ) = \mathbb{E}[ \min(râ‚œ(Ï€â‚œ) Aâ‚œ, \text{clip}(râ‚œ(Ï€â‚œ), 1 - Îµ, 1 + Îµ) Aâ‚œ)]
    $$

    Where:

    - $A_t$: Advantage function â€” how much better an action is than expected.  
    - $\epsilon$: Clipping threshold (typically 0.1â€“0.3).  

---

### 6. Value Function, Advantage, and Reward Computation

!!! example "ðŸ“— Supporting Mathematical Components"
    #### 6.1 Cumulative Reward (Return)
    $$
    Râ‚œ = \sum_{k=0}^{âˆž} Î³^k râ‚œâ‚Šâ‚–
    $$

    - \(râ‚œ\): reward received at time \(t\) (from reward model).  
    - \(Î³\): discount factor (typically 0.95â€“0.99).  

    In RLHF, responses are often short, so sequence-level rewards are used.

    #### 6.2 Value Function
    $$
    Vâ‚œ(sâ‚œ) â‰ˆ \mathbb{E}[Râ‚œ | sâ‚œ]
    $$

    Value loss:

    $$
    L^{value}(Ï€â‚œ) = \frac{1}{2} (Vâ‚œ(sâ‚œ) - Râ‚œ)^2
    $$

    #### 6.3 Advantage Function
    $$
    Aâ‚œ = Râ‚œ - Vâ‚œ(sâ‚œ)
    $$

    Optionally, **Generalized Advantage Estimation (GAE)**:

    $$
    Aâ‚œ = \sum_{l=0}^{âˆž} (Î³Î»)^l Î´â‚œâ‚Šâ‚—
    $$

    Where:

    - \(Î´â‚œ = râ‚œ + Î³ Vâ‚œ(sâ‚œâ‚Šâ‚) âˆ’ Vâ‚œ(sâ‚œ)\)  
    - \(Î»\) = GAE smoothing factor (0.9â€“0.97)

    #### 6.4 Entropy Bonus
    $$
    L^{entropy}(Ï€â‚œ) = - \sum_a Ï€â‚œ(a|sâ‚œ) \log Ï€â‚œ(a|sâ‚œ)
    $$

    Promotes exploration and diversity in responses.

    #### 6.5 Combined DRPO Loss
    $$
    L_{total}(Ï€â‚œ) = -L^{DRPO}(Ï€â‚œ) + câ‚ \cdot L^{value}(Ï€â‚œ) - câ‚‚ \cdot H[Ï€â‚œ]
    $$

    Where $H[Ï€â‚œ]$ is the entropy term; $câ‚, câ‚‚$ control loss weighting.

---

### 7. Iterative DRPO Update
1. Generate response with policy model.  
2. Compute reward using reward model.  
3. Compute log probabilities (old vs new policy).  
4. Estimate value using value head.  
5. Compute advantage.  
6. Update policy using **clipped surrogate loss**.  
7. Update value function.  
8. Apply entropy bonus.  
9. Update reference model.

> âœ… DRPO updates only when new behavior is better and within a controlled region, ensuring valid reasoning lengths are rewarded.

---

### 8. Implementation Example (Pseudocode)

```python

for prompt in prompts:
    response = policy_model.generate(prompt)
    reward = reward_model(prompt, response)
    
    logprobs_old = ref_model.logprobs(prompt, response)
    logprobs_new = policy_model.logprobs(prompt, response)
    
    value = value_head(prompt)
    
    advantage = reward - value  # sequence-level
    
    ratio = torch.exp(logprobs_new - logprobs_old)
    
    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)
    
    policy_loss = -torch.mean(torch.min(ratio * advantage, clipped_ratio * advantage))
    
    value_loss = 0.5 * (value - reward) ** 2
    
    entropy = -torch.sum(torch.exp(logprobs_new) * logprobs_new)
    entropy_coeff = 0.01
    
    total_loss = policy_loss + 0.5 * value_loss - entropy_coeff * entropy
    
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    ref_model.load_state_dict(policy_model.state_dict())

```

### 9. Limitations and Challenges

1. **KL Divergence Sensitivity**: Requires KL penalty to prevent divergence.  
2. **High Training Cost**: Multiple models increase compute.  
3. **Reward Hacking**: LLM may over-optimize for reward model.  
4. **Sparse/Noisy Rewards**: One reward per sequence can hinder credit assignment.  
5. **Credit Assignment**: Per-token updates with sequence-level rewards may be ambiguous.  
6. **Exploration vs Alignment**: Balancing safe exploration is challenging.  
7. **Implementation Complexity**: Multiple models and hyperparameter tuning required.

---

### 10. Summary

| Component             | Role                                  | Example |
|-----------------------|--------------------------------------|---------|
| **Policy Model (LLM)** | Generates responses                   | GPT-3, LLaMA-2 |
| **Reward Model**       | Scores outputs based on preferences  | Fine-tuned classifier |
| **DRPO Algorithm**     | Updates policy with decoupled reward | Training loop |
| **KL Penalty**         | Prevents over-deviation              | Regularization |
| **Goal**               | Align LLM behavior with human intent | Helpful, safe, accurate reasoning |
