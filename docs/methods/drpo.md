# üéØ Direct Reward Policy Optimization (DRPO) ‚Äî Offline Reward-Weighted Alignment

### 1. Overview

**Direct Reward Policy Optimization (DRPO)** is an algorithm designed to fine-tune **Large Language Models (LLMs)** using reward signals‚Äîwithout requiring an online reinforcement learning (RL) loop.
It directly leverages reward model scores as weights in a supervised training objective, offering a **simpler and more stable** alternative to **Proximal Policy Optimization (PPO)** while being **more expressive than Direct Preference Optimization (DPO)**.

---

### 2. The Big Picture: From PPO to DRPO

While **PPO-based RLHF** uses iterative, online reinforcement learning guided by a reward model, **DRPO** turns this process into a **single offline optimization step**.

| Stage   | PPO-Based RLHF                         | DRPO-Based Alignment         |
| ------- | -------------------------------------- | ---------------------------- |
| 1Ô∏è‚É£ SFT | Train base LLM on human demonstrations | ‚úÖ Same                       |
| 2Ô∏è‚É£ RM  | Train reward model on preference data  | ‚úÖ Same                       |
| 3Ô∏è‚É£ RL  | Fine-tune using PPO + rewards          | ‚úÖ Replaced by DRPO objective |

This makes DRPO **computationally cheaper**, **offline**, and **more stable**, while retaining reward sensitivity.

---

### 3. Intuitive Understanding

Imagine training an assistant:

* **PPO:** The assistant generates an answer ‚Üí a teacher scores it numerically ‚Üí the model updates using RL.
* **DRPO:** The assistant looks at many answers already scored ‚Üí learns to favor high-reward answers through weighted likelihood training.

Thus, DRPO bypasses the complex RL loop but still uses explicit reward signals for alignment.

---

### 4. Training Data and Setup

Each DRPO training example consists of $(x, y, r)$

where:

* $x$: Prompt or input query
* $y$: Candidate response (generated by the base model)
* $r$: Scalar reward value from the reward model

The model is fine-tuned to increase the likelihood of high-reward responses while staying close to a **reference model** $\pi_{\text{ref}}$ (often the SFT model).

---

### 5. DRPO Formulation

!!! example "üìò Mathematical Formulation"

    #### 5.1. Objective Function

    DRPO formulates offline alignment as a **reward-weighted log-likelihood objective** with KL regularization:

    $$
    L_{\mathrm{DRPO}}(\theta)
    = -\mathbb{E}_{(x, y)} \left[
    r(x, y) \, \log \pi_\theta(y|x)
    \right]
    + \beta \, D_{\mathrm{KL}}\!\left( \pi_\theta(\cdot|x) \, \| \, \pi_{\text{ref}}(\cdot|x) \right)
    $$

    where:

    * $\pi_\theta$: Trainable policy model  
    * $\pi_{\text{ref}}$: Reference (frozen) model  
    * $r(x, y)$: Reward score from the reward model  
    * $\beta$: KL regularization coefficient controlling drift from the reference model

    ---

    #### 5.2. Intuition
    
    DRPO can be viewed as **reward-weighted maximum likelihood estimation (MLE)** ‚Äî it learns a distribution that assigns higher probability mass to high-reward samples.
    
    The KL term maintains stability and prevents the model from overfitting or diverging too far from the reference model.

    Implicitly, this mirrors the PPO objective:
    $$
    L_{\mathrm{PPO}} \approx \mathbb{E}\!\left[
    A_t \frac{\pi_\theta(y_t|x_t)}{\pi_{\text{old}}(y_t|x_t)} - \beta \, D_{\mathrm{KL}}(\pi_\theta || \pi_{\text{old}})
    \right]
    $$
    but replaces *advantage estimates* $A_t$ with *offline rewards* $r(x, y)$ and removes the online loop.

    ---

    #### 5.3. Practical Implementation Notes / Gotchas

    * **Reference model is frozen** ‚Äî no gradient flow into $\pi_{\text{ref}}$.  
    * **Reward normalization** ‚Äî normalize $r(x, y)$ across batches to prevent gradient explosion.  
    * **Sequence log-probabilities** ‚Äî compute $\log \pi(y|x)$ at sequence level (sum or mean over tokens).  
    * **KL monitoring** ‚Äî track drift between $\pi_\theta$ and $\pi_{\text{ref}}$ during training.  
    * **Œ≤ (beta)** ‚Äî higher Œ≤ keeps the model closer to the base; typical range: 0.1‚Äì0.5.  
    * **Numerical stability** ‚Äî use stable PyTorch ops like `F.kl_div` or `F.log_softmax` to avoid underflow.  
    * **Dataset quality** ‚Äî ensure sufficient diversity in reward-labeled responses to avoid overfitting.

    ---

    #### 5.4. Key Takeaways

    * DRPO avoids online rollouts ‚Äî all data is precomputed.  
    * Retains **reward awareness** (unlike DPO) while remaining **stable** (unlike PPO).  
    * Functions as a **bridge** between supervised fine-tuning and reinforcement learning.  
    * Efficient for large-scale alignment when PPO is too costly.

---

### 6. Implementation Example (Pseudocode)

```python
for (prompt, response, reward) in reward_dataset:
    logp = model.logprobs(prompt, response)
    logp_ref = ref_model.logprobs(prompt, response)
    
    kl = (logp - logp_ref).mean()
    loss = -(reward * logp).mean() + beta * kl

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

### 7. DRPO vs. Other Methods

#### 7.1. DRPO vs. PPO

| Aspect        | PPO                      | DRPO                    |
| ------------- | ------------------------ | ----------------------- |
| Training loop | Online, iterative        | Offline, single-pass    |
| Reward usage  | Advantage-based, dynamic | Reward-weighted, static |
| Exploration   | Strong (on-policy)       | Limited (SFT data only) |
| Stability     | Unstable without tuning  | Very stable             |
| Cost          | High                     | 5‚Äì20√ó cheaper           |
| Performance   | Slightly higher          | ~90‚Äì95% of PPO          |

---

#### 7.2. DRPO vs. DPO

| Aspect       | DPO                    | DRPO                        |
| ------------ | ---------------------- | --------------------------- |
| Input        | Preference pairs       | Reward scores               |
| Feedback     | Binary preferences     | Scalar rewards              |
| Loss         | Logistic (pairwise)    | Weighted log-likelihood     |
| Reward model | Not needed             | Required                    |
| Exploration  | None                   | Some (via reward diversity) |
| Behavior     | Learns which is better | Learns how much better      |

---

### 8. Limitations and Challenges

#### üìâ 1. Reward Model Bias

DRPO inherits any biases or inaccuracies from the reward model, which may skew optimization.

#### üîÑ 2. Limited Exploration

Since DRPO is offline, it can only improve on responses generated by the base (SFT) model.

#### ‚öñÔ∏è 3. KL Tuning Sensitivity

Improper Œ≤ settings can lead to over-fitting (low Œ≤) or under-fitting (high Œ≤).

#### üß† 4. Reward Over-fitting

Over-optimizing for the reward model can cause **reward hacking** ‚Äî responses that exploit model scoring patterns.

#### üíæ 5. Data Requirement

Requires a sufficiently large and diverse dataset of reward-labeled responses for robust alignment.

---

### 9. Summary Table

| Component              | Role                                        | Example                       |
| ---------------------- | ------------------------------------------- | ----------------------------- |
| **Policy Model (LLM)** | Learns weighted preferences                 | `GPT-3`, `Llama-2`            |
| **Reference Model**    | Provides baseline probabilities             | SFT model                     |
| **Reward Model**       | Provides scalar scores for responses        | RM trained from human prefs   |
| **DRPO Objective**     | Reward-weighted log-likelihood + KL penalty | Offline alignment objective   |
| **Œ≤ Parameter**        | Controls proximity to reference             | Tuning hyperparameter         |
| **Goal**               | Efficient reward-based alignment            | Stable, cost-effective tuning |

---

### 10. When to Use DRPO

* You already have a **trained reward model**.
* You need **offline**, **stable**, and **cost-efficient** alignment.
* You want to **bridge** between SFT and PPO without running expensive RL loops.
* Ideal for **intermediate or large-scale pre-alignment** before PPO fine-tuning.

---