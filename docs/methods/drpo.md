# DRPO (Direct Reward Policy Optimization) for LLM Fine-Tuning

This document provides a detailed overview of **DRPO**, a method for fine-tuning large language models (LLMs) using reward-based alignment. It also compares DRPO with other approaches like **PPO** and **DPO**, highlighting workflow, advantages, limitations, and performance considerations.

---

## Table of Contents

- [Overview](#overview)
- [Key Idea](#key-idea)
- [Training Workflow](#training-workflow)
- [Where Rewards Come From](#where-rewards-come-from)
- [Differences from PPO](#differences-from-ppo)
- [Differences from DPO](#differences-from-dpo)
- [Advantages of DRPO](#advantages-of-drpo)
- [Limitations / Issues](#limitations--issues)
- [Performance Comparison](#performance-comparison)
- [When to Use DRPO](#when-to-use-drpo)
- [Workflow Diagram](#workflow-diagram)

---

## Overview

**Direct Reward Policy Optimization (DRPO)** is an **offline, reward-weighted fine-tuning** method for LLMs. It is inspired by reinforcement learning techniques like PPO but avoids the **expensive and unstable online RL loop**.

DRPO is used to align LLMs to produce helpful, safe, and high-quality responses based on feedback captured in a **reward model (RM)**.

---

## Key Idea

- Start with a **base model** (often SFT-fine-tuned).  
- Generate multiple responses per prompt using this base model.  
- Use a **reward model** to assign a scalar reward to each response.  
- Fine-tune the model using a **weighted log-likelihood loss**, encouraging high-reward responses while applying a **KL penalty** to maintain closeness to the base model.

**DRPO Loss Function**:

\[
L(\pi) = - \mathbb{E}_{(x, y)} [ r(x, y) \cdot \log \pi(y|x) ] + \beta KL(\pi || \pi_0)
\]

Where:  
- \(r(x, y)\) = reward score from the reward model  
- \(\pi\) = fine-tuned model  
- \(\pi_0\) = base (SFT) model  
- \(\beta\) = KL coefficient

---

## Training Workflow

### DRPO (Offline)

1. Generate a **fixed dataset** of (prompt, response) pairs from the **SFT model**.  
2. Compute **reward scores** for all responses using the **reward model**.  
3. Train the LLM with **weighted log-likelihood**, using the reward as weights and a KL penalty.  
4. No iterative online rollouts are needed.

### PPO (Online RLHF)

1. Initialize policy with SFT model.  
2. Generate responses **online** with the current policy.  
3. Score responses using the reward model.  
4. Compute advantages and perform **policy gradient updates**.  
5. Repeat the loop for many iterations — the policy evolves, generating new responses each step.

**Key Difference:** DRPO uses **offline, precomputed data**, while PPO relies on **online, on-policy exploration**.

---

## Where Rewards Come From

- Rewards are computed from a **reward model** trained on human preference data.  
- Candidate responses are generated by the **SFT model**.  
- The reward model assigns a scalar value for each (prompt, response) pair, which DRPO uses as **weights** during training.

**Dataset Example:**

| Prompt | Response | Reward |
|--------|---------|--------|
| "Explain photosynthesis." | "Plants use sunlight to make energy..." | 0.92 |
| "Explain photosynthesis." | "It’s about light and plants." | 0.45 |

---

## Differences from PPO

| Aspect | PPO | DRPO |
|--------|-----|------|
| Training loop | Online, iterative RL | Offline, single-pass fine-tuning |
| Reward usage | Used online for policy gradient | Used offline as weighting for supervised loss |
| Exploration | Strong — policy generates new outputs each step | Limited — only SFT-generated responses |
| Stability | Can be unstable | Stable like supervised learning |
| Training cost | High (many rollouts, multiple gradient steps) | Low (single dataset pass) |
| Performance | Slightly better alignment | Close to PPO but slightly lower |

---

## Differences from DPO

| Aspect | DPO | DRPO |
|--------|-----|------|
| Input data | Preference pairs (chosen vs rejected) | Scalar reward values |
| Loss | Logistic loss on pairwise comparisons | Weighted log-likelihood on reward |
| Feedback | Human preferences | Reward model scores |
| Behavior | Learns which output is better | Learns how much better an output is |

---

## Advantages of DRPO

- **Stable training**: Avoids PPO’s gradient instability.  
- **Faster & cheaper**: Single-pass offline training.  
- **Simple implementation**: Just a modified supervised fine-tuning loss.  
- **Good alignment**: Reward-aware fine-tuning can improve response quality efficiently.

---

## Limitations / Issues

- **Reward model bias**: DRPO inherits any biases in the reward model.  
- **Limited exploration**: Only improves on existing SFT responses.  
- **Reward overfitting**: Risk of “reward hacking” if the reward model is not robust.  
- **KL tuning required**: Poor KL weight can lead to underfitting or overfitting.  
- **Data requirement**: Needs a sufficiently diverse set of reward-scored responses.

---

## Performance Comparison

| Metric | PPO | DRPO |
|--------|-----|------|
| Alignment quality | Slightly higher | Slightly lower, ~90–95% of PPO |
| Training stability | Can be unstable | Very stable |
| Training cost | Very high | 5–20× faster |
| Exploration | Strong | Limited |
| Practical use | Final alignment pass in large LLMs | Efficient intermediate alignment |

---

## When to Use DRPO

- You already have a **trained reward model**.  
- You want **fast, stable, and cost-effective alignment**.  
- You want to fine-tune using **existing SFT responses** rather than running expensive online RL loops.  
- Useful for **intermediate-stage alignment** before deploying PPO or other full RL methods.

---

## Workflow Diagram

```text
      SFT Model (π₀)
           |
           | Generate responses (offline or online)
           v
     Candidate Responses
           |
           | Score with Reward Model (R)
           v
   ------------------------
   |                      |
DRPO (Offline)        PPO (Online)
Weighted Log-Likelihood  Policy Gradient Updates
+ KL Penalty             + KL Penalty
Single-pass training      Iterative rollouts & training
Limited exploration       Strong exploration
Stable & fast             Slower, high cost, slightly higher alignment
```

---

## References

- Christiano, P., et al. (2017). *Deep Reinforcement Learning from Human Preferences.*  
- Bai, Y., et al. (2022). *Training a Helpful and Harmless Assistant with RLHF.*  
- Stiennon, N., et al. (2022). *Learning to summarize with human feedback.*

---

