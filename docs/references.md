* *Rafailov et al., “Direct Preference Optimization: Your Language Model is Secretly a Reward Model,” 2023*
* *Hugging Face TRL Documentation — DPOTrainer*
* *“Simplifying RLHF with DPO,” Hugging Face Blog, 2023*
* Christiano, P., et al. (2017). *Deep Reinforcement Learning from Human Preferences.*
* Bai, Y., et al. (2022). *Training a Helpful and Harmless Assistant with RLHF.*
* Rafailov, R., et al. (2023). *Direct Preference Optimization: Your Language Model is Secretly a Reward Model.*
* Stiennon, N., et al. (2022). *Learning to Summarize with Human Feedback.*
