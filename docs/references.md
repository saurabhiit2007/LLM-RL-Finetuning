# üìö References

### üßÆ 1. PPO - *Proximal Policy Optimization*

* **Schulman et al. (2017)** ‚Äî *Proximal Policy Optimization Algorithms.* [[arXiv:1707.06347]](https://arxiv.org/abs/1707.06347)
* **Adaptive-ML Blog (2023)** ‚Äî *From Zero to PPO: Understanding the Path to Helpful AI Models.* [[Link]](https://www.adaptive-ml.com/post/from-zero-to-ppo)
* **Secrets of RLHF in LLMs (2023)** ‚Äî *Part I: PPO Explained in Detail.* [[arXiv:2307.04964]](https://arxiv.org/abs/2307.04964)

---

### üéØ 2. DPO - *Direct Preference Optimization*

* **Rafailov et al. (2023)** ‚Äî *Direct Preference Optimization: Your Language Model is Secretly a Reward Model.* [[arXiv:2305.18290]](https://arxiv.org/abs/2305.18290)
* **Hugging Face Blog (2023)** ‚Äî *Simplifying RLHF with DPO.* [[Blog]](https://huggingface.co/blog/ariG23498/rlhf-to-dpo)
* **Implementation Repo** ‚Äî [eric-mitchell/direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization)

---

### üîÅ 3. GRPO - *Grouped Relative Policy Optimization*

* **Shao et al. (2024)** ‚Äî *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.* (Introduces GRPO) [[arXiv:2402.03300]](https://arxiv.org/abs/2402.03300)
* **Mroueh et al. (2025)** ‚Äî *Revisiting Group Relative Policy Optimization.* [[arXiv:2505.22257]](https://arxiv.org/abs/2505.22257)
* **Samia Sahin (2025)** ‚Äî *The Math Behind DeepSeek ‚Äî GRPO Explained.* [[Medium]](https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba)

---

### üîÅ 3. DRPO - *Decoupled Rewards Policy Optimization*

* **Li et al. (2025)** - *DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization.* [DRPO Paper](https://arxiv.org/abs/2510.04474)

---

### ‚öñÔ∏è 4. KL Penalty

* **APXML Guide (2023)** ‚Äî *KL Divergence Penalty in RLHF.* [[Article]](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-26-reinforcement-learning-human-feedback-rlhf/kl-divergence-penalty-rlhf)

---

### üöÄ 5. DeepSeek RL ‚Äî *Reinforcement Learning for Reasoning*

* **Guo et al. (2025)** ‚Äî *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.* [[Link]](https://arxiv.org/abs/2501.12948)
* **Analytics Vidhya (2025)** ‚Äî *LLM Optimization: GRPO, PPO, and DPO.* [[Blog]](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization)

---

### üß† 6. Reward Hacking & Specification Gaming

* **Amodei et al. (2016)** ‚Äî *Concrete Problems in AI Safety.* [[arXiv:1606.06565]](https://arxiv.org/abs/1606.06565)

---

### üß© Others

* **Ouyang et al. (2022)** ‚Äî *Training Language Models to Follow Instructions with Human Feedback (InstructGPT).* [[arXiv:2203.02155]](https://arxiv.org/abs/2203.02155)
* **Bai et al. (2022)** ‚Äî *Training a Helpful and Harmless Assistant with RLHF.* [[arXiv:2204.05862]](https://arxiv.org/abs/2204.05862)
