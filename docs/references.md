* *Rafailov et al., “Direct Preference Optimization: Your Language Model is Secretly a Reward Model,” 2023*
* *Hugging Face TRL Documentation — DPOTrainer*
* *“Simplifying RLHF with DPO,” Hugging Face Blog, 2023*
* Christiano, P., et al. (2017). *Deep Reinforcement Learning from Human Preferences.*
* Bai, Y., et al. (2022). *Training a Helpful and Harmless Assistant with RLHF.*
* Rafailov, R., et al. (2023). *Direct Preference Optimization: Your Language Model is Secretly a Reward Model.*
* Stiennon, N., et al. (2022). *Learning to Summarize with Human Feedback.*
* Shao et al., **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models**, 2024.  
  [[arXiv:2402.03300]](https://arxiv.org/abs/2402.03300)
* Guo et al., **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**, *Nature*, 2025.  
  [[Nature link]](https://www.nature.com/articles/s41586-025-09422-z)
* Samia Sahin, *The Math Behind DeepSeek — GRPO Explained*, Medium (2025).  
  [[Medium Article]](https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba)
